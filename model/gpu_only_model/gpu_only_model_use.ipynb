{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains code for using the gpu only model.\n",
    "The conda environment required for running this notebook can be installed and activated by running the following on the command line from within this folder: \n",
    "<code>conda env create -f evolution_env_gpu_only.yml </code>  \\\n",
    "<code>conda activate seq</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_conditions = 'pTpA_Glu' #  'SC_Ura''pTpA_Glu' \n",
    "\n",
    "\n",
    "def closest_point(node, nodes):\n",
    "    nodes = np.asarray(nodes)\n",
    "    deltas = nodes - node\n",
    "    dist_2 = np.einsum('ij,ij->i', deltas, deltas)\n",
    "    return np.argmin(dist_2)\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import pickle , csv ,  argparse , os ,random , multiprocessing , copy , scipy , multiprocessing\n",
    "import tensorflow as tf, sys, numpy as np, h5py, pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from deap import creator, base, tools, algorithms\n",
    "\n",
    "from os.path import splitext,exists,dirname,join,basename\n",
    "\n",
    "\n",
    "def seq2feature(data,mapper,worddim):\n",
    "    \n",
    "    transformed = np.zeros([data.shape[0],1,len(data[0]),4] , dtype=np.bool )\n",
    "    for i in range(data.shape[0]) :\n",
    "        for j,k in enumerate(data[i]):\n",
    "            #print j,k\n",
    "            transformed[i,0,j] = mapper[k] \n",
    "            #print mapper[k]\n",
    "    return transformed\n",
    "    \n",
    "\n",
    "\n",
    "def get_rc(A):\n",
    "    A_r = np.flip(A,2)\n",
    "    A = A_r\n",
    "    A_onehot = np.array([1,0,0,0] ,  dtype=np.bool)\n",
    "    C_onehot = np.array([0,1,0,0] ,  dtype=np.bool)\n",
    "    G_onehot = np.array([0,0,1,0] ,  dtype=np.bool)\n",
    "    T_onehot = np.array([0,0,0,1] ,  dtype=np.bool)\n",
    "    N_onehot = np.array([0,0,0,0] ,  dtype=np.bool)\n",
    "\n",
    "    for i in range(A.shape[0]) : \n",
    "        for j in range(A.shape[2]) : \n",
    "            if np.array_equal(A[i][0][j] , A_onehot) :\n",
    "                A[i][0][j] = T_onehot\n",
    "            elif np.array_equal(A[i][0][j] , C_onehot) :\n",
    "                A[i][0][j] = G_onehot\n",
    "            elif np.array_equal(A[i][0][j] , G_onehot) :\n",
    "                A[i][0][j] = C_onehot\n",
    "            elif np.array_equal(A[i][0][j] , T_onehot) :\n",
    "                A[i][0][j] = A_onehot\n",
    "            elif np.array_equal(A[i][0][j] , N_onehot) :\n",
    "                A[i][0][j] = N_onehot\n",
    "\n",
    "                \n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hamming_distance(s1, s2):\n",
    "    return sum(ch1 != ch2 for ch1,ch2 in zip(s1,s2))\n",
    "\n",
    "\n",
    "# this function modified from Effective Tensorflow: https://github.com/vahidk/EffectiveTensorflow#multi_gpu\n",
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    out_split=[]\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                out_split.append(fn(**{k : v[i] for k, v in in_splits.items()}))\n",
    "\n",
    "    out_splits=[]\n",
    "\n",
    "    out_splits.append(tf.concat([x[0] for x in out_split], axis=0))\n",
    "    out_splits.append(tf.stack([x[1] for x in out_split], axis=0))\n",
    "    return out_splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getPredictions(sequences_raw , sess , graph, X, X_rc, Y, model_cost, model_output ):\n",
    "\n",
    "    BASEORDER = list(\"ACGT\")\n",
    "    A_onehot = np.array([1,0,0,0] ,  dtype=np.bool)\n",
    "    C_onehot = np.array([0,1,0,0] ,  dtype=np.bool)\n",
    "    G_onehot = np.array([0,0,1,0] ,  dtype=np.bool)\n",
    "    T_onehot = np.array([0,0,0,1] ,  dtype=np.bool)\n",
    "    N_onehot = np.array([0,0,0,0] ,  dtype=np.bool)\n",
    "\n",
    "    mapper = {'A':A_onehot,'C':C_onehot,'G':G_onehot,'T':T_onehot,'N':N_onehot}\n",
    "    worddim = len(mapper['A'])\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "    if(np.ndim(sequences_raw)==1) : \n",
    "        if(len(sequences_raw[0])==80) : \n",
    "            batchX = [];\n",
    "            for ind in range(len(sequences_raw)) :\n",
    "                individual_pred = ''.join(['T','G','C','A','T','T','T','T','T','T','T','C','A','C','A','T','C'] + sequences_raw[ind] + ['G','G','T','T','A','C','G','G','C','T','G','T','T'] )\n",
    "                batchX.append(individual_pred)\n",
    "\n",
    "        else :\n",
    "            batchX = sequences_raw\n",
    "\n",
    "        _teX = seq2feature(np.asarray(batchX))#,mapper,worddim)\n",
    "        _teX_rc = get_rc(list(_teX)) \n",
    "        _teY = np.zeros([np.shape(_teX)[0] , 1] , dtype=float)\n",
    "\n",
    "        \n",
    "    elif(np.ndim(sequences_raw)==2) : \n",
    "        if(len(sequences_raw[0])==80) : \n",
    "            batchX = [];\n",
    "            for ind in range(len(sequences_raw)) :\n",
    "                individual_pred = ''.join(['T','G','C','A','T','T','T','T','T','T','T','C','A','C','A','T','C'] + sequences_raw[ind] + ['G','G','T','T','A','C','G','G','C','T','G','T','T'] )\n",
    "                batchX.append(individual_pred)\n",
    "\n",
    "        else :\n",
    "            batchX = sequences_raw\n",
    "\n",
    "        _teX = seq2feature(np.asarray(batchX))#,mapper,worddim)\n",
    "        _teX_rc = get_rc(list(_teX)) \n",
    "        _teY = np.zeros([np.shape(_teX)[0] , 1] , dtype=float)\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    batch_size = (np.shape(_teX)[0])\n",
    "    #print batch_size\n",
    "    jump_size =4096\n",
    "    (q,r) = divmod(batch_size , jump_size)\n",
    "    #print q , r \n",
    "    i=0\n",
    "    teX_output = []\n",
    "    while(i <= q ) :\n",
    "        if(i< q  ) :\n",
    "            #with tf.device(tf.DeviceSpec(device_type=\"CPU\", device_index= 3 )):\n",
    "            temp_result_step1=sess.run([model_output], feed_dict={X: _teX[jump_size*i:jump_size*i+jump_size,:], X_rc: _teX_rc[jump_size*i:jump_size*i+jump_size,:]  ,Y: _teY[jump_size*i:jump_size*i+jump_size,:]}) \n",
    "            temp_result_step2=[float(x) for x in temp_result_step1[0]]\n",
    "            #print temp_result_step2\n",
    "\n",
    "            teX_output = teX_output + temp_result_step2\n",
    "            i = i+1\n",
    "\n",
    "        elif (i==q) :\n",
    "            #with tf.device('/device:CPU:3'):\n",
    "            temp_result_step1 = sess.run([model_output], feed_dict={X: _teX[jump_size*i:,:], X_rc: _teX_rc[jump_size*i:,:]  ,Y: _teY[jump_size*i:,:]})\n",
    "            temp_result_step2=[float(x) for x in temp_result_step1[0]]\n",
    "            #print \"here\"\n",
    "            teX_output = teX_output + temp_result_step2\n",
    "            i = i+1\n",
    "\n",
    "\n",
    "            #tf.reset_default_graph()\n",
    "    return teX_output\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "_best_model_file = join('..',model_conditions,'models','best_model.ckpt')\n",
    "\n",
    "_best_model_file_hyper = join('..',model_conditions,'models','hyper_search','best_model.ckpt')\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "sess.list_devices()\n",
    "init=tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init)\n",
    "\n",
    "new_saver = tf.train.import_meta_graph('%s.meta'%_best_model_file)\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(join('..',model_conditions,'models')))\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = graph.get_tensor_by_name('Placeholder:0')\n",
    "X_rc = graph.get_tensor_by_name('Placeholder_1:0')\n",
    "Y = graph.get_tensor_by_name('Placeholder_2:0')\n",
    "\n",
    "model_cost = graph.get_tensor_by_name('out/model_cost:0')\n",
    "model_output = graph.get_tensor_by_name('out/model_output:0')\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "if (1) : \n",
    "    with open('../pTpA_Glu/justSeqs_native_HQ.sequencedRegion110.NATIVE_ONLY.txt') as f: ### Replace with path to the sequences whose expression you want to compute\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        d = list(reader)\t\n",
    "\n",
    "    sequences = [di[1] for di in d]\n",
    "    for i in range(0,len(sequences)) : \n",
    "        if (len(sequences[i]) > 110) :\n",
    "            sequences[i] = sequences[i][-110:]\n",
    "        if (len(sequences[i]) < 110) : \n",
    "            #print i\n",
    "            while (len(sequences[i]) < 110) :\n",
    "                sequences[i] = 'N'+sequences[i]\n",
    "\n",
    "\n",
    "    seqdata = list(np.asarray(sequences))\n",
    "    native_sequences=seqdata\n",
    "\n",
    "if(0) : ### Only useful if you have processed data from the preprocessing script in this folder\n",
    "    ## Load the data matrix \n",
    "    with h5py.File(join('..',model_conditions,'_teY.h5'), 'r') as hf:\n",
    "        _teY = hf['_teY'][:]\n",
    "    #expressions.shape\n",
    "\n",
    "    ## Load the sequences \n",
    "    with h5py.File(join('..',model_conditions,'_teX.h5'), 'r') as hf:\n",
    "        _teX = hf['_teX'][:]\n",
    "    #onehot_sequences.shape\n",
    "\n",
    "    ## Load the rc sequences \n",
    "    with h5py.File(join('..',model_conditions,'_teX_rc.h5'), 'r') as hf:\n",
    "        _teX_rc = hf['_teX_rc'][:]\n",
    "\n",
    "    seqdata = _teX\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Used to be a function\n",
    "n=1024\n",
    "full_seqdata_length= np.shape(seqdata)[0]\n",
    "sequence_length = len(seqdata[0]) - 30 # 80 , 17 on 5' end and 13 on 3' end \n",
    "\n",
    "seqs = np.asarray(seqdata)     # you need to convert to np array to index the sequences properly\n",
    "\n",
    "np.random.seed(1234)\n",
    "seqs = seqs[np.random.choice(full_seqdata_length,n)]\n",
    "\n",
    "sequences = seqs \n",
    "expressions = np.asarray(getPredictions(original_sequences , sess , graph, X, X_rc, Y, model_cost, model_output) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#The variable `expressions` contains the expression estimate for the `sequences`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:seq]",
   "language": "python",
   "name": "conda-env-seq-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
