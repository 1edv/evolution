{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains code for using the gpu only model.\n",
    "The conda environment required for running this notebook can be installed and activated by running the following on the command line from within this folder: \n",
    "<code>conda env create -f evolution_env_gpu_only.yml </code>  \\\n",
    "<code>conda activate seq</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../pTpA_Glu/models/best_model.ckpt.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ebd79cb67d74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m \u001b[0mnew_saver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s.meta'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0m_best_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0mnew_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_conditions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ahg/regevdata/users/edv/software/anaconda3/envs/seq/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1672\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[1;32m   1673\u001b[0m   return _import_meta_graph_with_return_elements(\n\u001b[0;32m-> 1674\u001b[0;31m       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n\u001b[0m\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ahg/regevdata/users/edv/software/anaconda3/envs/seq/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1684\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1685\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1687\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ahg/regevdata/users/edv/software/anaconda3/envs/seq/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.pyc\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    631\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../pTpA_Glu/models/best_model.ckpt.meta does not exist."
     ]
    }
   ],
   "source": [
    "\n",
    "model_conditions = 'pTpA_Glu' #  'SC_Ura''pTpA_Glu' \n",
    "\n",
    "\n",
    "def closest_point(node, nodes):\n",
    "    nodes = np.asarray(nodes)\n",
    "    deltas = nodes - node\n",
    "    dist_2 = np.einsum('ij,ij->i', deltas, deltas)\n",
    "    return np.argmin(dist_2)\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import pickle , csv ,  argparse , os ,random , multiprocessing , copy , scipy , multiprocessing\n",
    "import tensorflow as tf, sys, numpy as np, h5py, pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from deap import creator, base, tools, algorithms\n",
    "\n",
    "from os.path import splitext,exists,dirname,join,basename\n",
    "\n",
    "\n",
    "def seq2feature(data,mapper,worddim):\n",
    "    \n",
    "    transformed = np.zeros([data.shape[0],1,len(data[0]),4] , dtype=np.bool )\n",
    "    for i in range(data.shape[0]) :\n",
    "        for j,k in enumerate(data[i]):\n",
    "            #print j,k\n",
    "            transformed[i,0,j] = mapper[k] \n",
    "            #print mapper[k]\n",
    "    return transformed\n",
    "    \n",
    "\n",
    "\n",
    "def get_rc(A):\n",
    "    A_r = np.flip(A,2)\n",
    "    A = A_r\n",
    "    A_onehot = np.array([1,0,0,0] ,  dtype=np.bool)\n",
    "    C_onehot = np.array([0,1,0,0] ,  dtype=np.bool)\n",
    "    G_onehot = np.array([0,0,1,0] ,  dtype=np.bool)\n",
    "    T_onehot = np.array([0,0,0,1] ,  dtype=np.bool)\n",
    "    N_onehot = np.array([0,0,0,0] ,  dtype=np.bool)\n",
    "\n",
    "    for i in range(A.shape[0]) : \n",
    "        for j in range(A.shape[2]) : \n",
    "            if np.array_equal(A[i][0][j] , A_onehot) :\n",
    "                A[i][0][j] = T_onehot\n",
    "            elif np.array_equal(A[i][0][j] , C_onehot) :\n",
    "                A[i][0][j] = G_onehot\n",
    "            elif np.array_equal(A[i][0][j] , G_onehot) :\n",
    "                A[i][0][j] = C_onehot\n",
    "            elif np.array_equal(A[i][0][j] , T_onehot) :\n",
    "                A[i][0][j] = A_onehot\n",
    "            elif np.array_equal(A[i][0][j] , N_onehot) :\n",
    "                A[i][0][j] = N_onehot\n",
    "\n",
    "                \n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hamming_distance(s1, s2):\n",
    "    return sum(ch1 != ch2 for ch1,ch2 in zip(s1,s2))\n",
    "\n",
    "\n",
    "# this function modified from Effective Tensorflow: https://github.com/vahidk/EffectiveTensorflow#multi_gpu\n",
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    out_split=[]\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                out_split.append(fn(**{k : v[i] for k, v in in_splits.items()}))\n",
    "\n",
    "    out_splits=[]\n",
    "\n",
    "    out_splits.append(tf.concat([x[0] for x in out_split], axis=0))\n",
    "    out_splits.append(tf.stack([x[1] for x in out_split], axis=0))\n",
    "    return out_splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getPredictions(sequences_raw , sess , graph, X, X_rc, Y, model_cost, model_output ):\n",
    "\n",
    "    BASEORDER = list(\"ACGT\")\n",
    "    A_onehot = np.array([1,0,0,0] ,  dtype=np.bool)\n",
    "    C_onehot = np.array([0,1,0,0] ,  dtype=np.bool)\n",
    "    G_onehot = np.array([0,0,1,0] ,  dtype=np.bool)\n",
    "    T_onehot = np.array([0,0,0,1] ,  dtype=np.bool)\n",
    "    N_onehot = np.array([0,0,0,0] ,  dtype=np.bool)\n",
    "\n",
    "    mapper = {'A':A_onehot,'C':C_onehot,'G':G_onehot,'T':T_onehot,'N':N_onehot}\n",
    "    worddim = len(mapper['A'])\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "    if(np.ndim(sequences_raw)==1) : \n",
    "        if(len(sequences_raw[0])==80) : \n",
    "            batchX = [];\n",
    "            for ind in range(len(sequences_raw)) :\n",
    "                individual_pred = ''.join(['T','G','C','A','T','T','T','T','T','T','T','C','A','C','A','T','C'] + sequences_raw[ind] + ['G','G','T','T','A','C','G','G','C','T','G','T','T'] )\n",
    "                batchX.append(individual_pred)\n",
    "\n",
    "        else :\n",
    "            batchX = sequences_raw\n",
    "\n",
    "        _teX = seq2feature(np.asarray(batchX))#,mapper,worddim)\n",
    "        _teX_rc = get_rc(list(_teX)) \n",
    "        _teY = np.zeros([np.shape(_teX)[0] , 1] , dtype=float)\n",
    "\n",
    "        \n",
    "    elif(np.ndim(sequences_raw)==2) : \n",
    "        if(len(sequences_raw[0])==80) : \n",
    "            batchX = [];\n",
    "            for ind in range(len(sequences_raw)) :\n",
    "                individual_pred = ''.join(['T','G','C','A','T','T','T','T','T','T','T','C','A','C','A','T','C'] + sequences_raw[ind] + ['G','G','T','T','A','C','G','G','C','T','G','T','T'] )\n",
    "                batchX.append(individual_pred)\n",
    "\n",
    "        else :\n",
    "            batchX = sequences_raw\n",
    "\n",
    "        _teX = seq2feature(np.asarray(batchX))#,mapper,worddim)\n",
    "        _teX_rc = get_rc(list(_teX)) \n",
    "        _teY = np.zeros([np.shape(_teX)[0] , 1] , dtype=float)\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    batch_size = (np.shape(_teX)[0])\n",
    "    #print batch_size\n",
    "    jump_size =4096\n",
    "    (q,r) = divmod(batch_size , jump_size)\n",
    "    #print q , r \n",
    "    i=0\n",
    "    teX_output = []\n",
    "    while(i <= q ) :\n",
    "        if(i< q  ) :\n",
    "            #with tf.device(tf.DeviceSpec(device_type=\"CPU\", device_index= 3 )):\n",
    "            temp_result_step1=sess.run([model_output], feed_dict={X: _teX[jump_size*i:jump_size*i+jump_size,:], X_rc: _teX_rc[jump_size*i:jump_size*i+jump_size,:]  ,Y: _teY[jump_size*i:jump_size*i+jump_size,:]}) \n",
    "            temp_result_step2=[float(x) for x in temp_result_step1[0]]\n",
    "            #print temp_result_step2\n",
    "\n",
    "            teX_output = teX_output + temp_result_step2\n",
    "            i = i+1\n",
    "\n",
    "        elif (i==q) :\n",
    "            #with tf.device('/device:CPU:3'):\n",
    "            temp_result_step1 = sess.run([model_output], feed_dict={X: _teX[jump_size*i:,:], X_rc: _teX_rc[jump_size*i:,:]  ,Y: _teY[jump_size*i:,:]})\n",
    "            temp_result_step2=[float(x) for x in temp_result_step1[0]]\n",
    "            #print \"here\"\n",
    "            teX_output = teX_output + temp_result_step2\n",
    "            i = i+1\n",
    "\n",
    "\n",
    "            #tf.reset_default_graph()\n",
    "    return teX_output\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "_best_model_file = join('..',model_conditions,'models','best_model.ckpt')\n",
    "\n",
    "_best_model_file_hyper = join('..',model_conditions,'models','hyper_search','best_model.ckpt')\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "sess.list_devices()\n",
    "init=tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init)\n",
    "\n",
    "new_saver = tf.train.import_meta_graph('%s.meta'%_best_model_file)\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(join('..',model_conditions,'models')))\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = graph.get_tensor_by_name('Placeholder:0')\n",
    "X_rc = graph.get_tensor_by_name('Placeholder_1:0')\n",
    "Y = graph.get_tensor_by_name('Placeholder_2:0')\n",
    "\n",
    "model_cost = graph.get_tensor_by_name('out/model_cost:0')\n",
    "model_output = graph.get_tensor_by_name('out/model_output:0')\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "if (1) : \n",
    "    with open('../pTpA_Glu/justSeqs_native_HQ.sequencedRegion110.NATIVE_ONLY.txt') as f: ### Replace with path to the sequences whose expression you want to compute\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        d = list(reader)\t\n",
    "\n",
    "    sequences = [di[1] for di in d]\n",
    "    for i in range(0,len(sequences)) : \n",
    "        if (len(sequences[i]) > 110) :\n",
    "            sequences[i] = sequences[i][-110:]\n",
    "        if (len(sequences[i]) < 110) : \n",
    "            #print i\n",
    "            while (len(sequences[i]) < 110) :\n",
    "                sequences[i] = 'N'+sequences[i]\n",
    "\n",
    "\n",
    "    seqdata = list(np.asarray(sequences))\n",
    "    native_sequences=seqdata\n",
    "\n",
    "if(0) : ### Only useful if you have processed data from the preprocessing script in this folder\n",
    "    ## Load the data matrix \n",
    "    with h5py.File(join('..',model_conditions,'_teY.h5'), 'r') as hf:\n",
    "        _teY = hf['_teY'][:]\n",
    "    #expressions.shape\n",
    "\n",
    "    ## Load the sequences \n",
    "    with h5py.File(join('..',model_conditions,'_teX.h5'), 'r') as hf:\n",
    "        _teX = hf['_teX'][:]\n",
    "    #onehot_sequences.shape\n",
    "\n",
    "    ## Load the rc sequences \n",
    "    with h5py.File(join('..',model_conditions,'_teX_rc.h5'), 'r') as hf:\n",
    "        _teX_rc = hf['_teX_rc'][:]\n",
    "\n",
    "    seqdata = _teX\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Used to be a function\n",
    "n=1024\n",
    "full_seqdata_length= np.shape(seqdata)[0]\n",
    "sequence_length = len(seqdata[0]) - 30 # 80 , 17 on 5' end and 13 on 3' end \n",
    "\n",
    "seqs = np.asarray(seqdata)     # you need to convert to np array to index the sequences properly\n",
    "\n",
    "np.random.seed(1234)\n",
    "seqs = seqs[np.random.choice(full_seqdata_length,n)]\n",
    "\n",
    "sequences = seqs \n",
    "expressions = np.asarray(getPredictions(original_sequences , sess , graph, X, X_rc, Y, model_cost, model_output) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#The variable `expressions` contains the expression estimate for the `sequences`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:seq]",
   "language": "python",
   "name": "conda-env-seq-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
