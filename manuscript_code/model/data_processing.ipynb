{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1225 21:36:00.404302 47717600894592 deprecation_wrapper.py:119] From ../aux.py:29: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "import aux \n",
    "from aux import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import argparse,pwd,os,numpy as np,h5py\n",
    "from os.path import splitext,exists,dirname,join,basename\n",
    "from os import makedirs\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from os.path import join,dirname,basename,exists,realpath\n",
    "from os import makedirs\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import sys,seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31349363/31349363 [00:19<00:00, 1606512.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31349363, 110, 4)\n"
     ]
    }
   ],
   "source": [
    "model_conditions = 'Glu'\n",
    "with open(os.path.join('..','..','data',model_conditions ,'training_data_'+model_conditions+'.txt')) as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\t\n",
    "\n",
    "    \n",
    "\n",
    "sequences = [di[0] for di in d]\n",
    "\n",
    "for i in tqdm(range(0,len(sequences))) : \n",
    "    if (len(sequences[i]) > 110) :\n",
    "        sequences[i] = sequences[i][-110:]\n",
    "    if (len(sequences[i]) < 110) : \n",
    "        while (len(sequences[i]) < 110) :\n",
    "            sequences[i] = 'N'+sequences[i]\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "# Function to embed sequences and another one to get reverse complements\n",
    "\n",
    "\n",
    "seqdata_transformed = seq2feature(sequences)\n",
    "print(seqdata_transformed.shape)\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join('..','..','data',model_conditions ,'onehot_sequences_bool.h5'), 'w') as hf:\n",
    "    hf.create_dataset(\"onehot_sequences_bool\",  data=seqdata_transformed)\n",
    "print(type(seqdata_transformed[0][0][0]))\n",
    "\n",
    "#Reverse complement sequences are saved at a later stage below\n",
    "\n",
    "\n",
    "## Now , Create The Data class label vectors and Store in the same h5py file\n",
    "expressions = [di[1] for di in d]\n",
    "expdata = np.asarray(expressions)\n",
    "expdata = expdata.astype('float')  \n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join('..','..','data',model_conditions ,'expression.h5'), 'w') as hf:\n",
    "    hf.create_dataset(\"expression\",  data=expdata)  \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################   \n",
    "################  \n",
    "#### Do the training test split below\n",
    "################\n",
    "################\n",
    "\n",
    "if 0:\n",
    "    ## Load the data matrix \n",
    "    with h5py.File(os.path.join('..','..','data',model_conditions ,'expression.h5'), 'r') as hf:\n",
    "        expressions = hf['expression'][:]\n",
    "    #expressions.shape\n",
    "\n",
    "    ## Load the sequences \n",
    "    with h5py.File(join('..','..','data',model_conditions ,'onehot_sequences_bool.h5'), 'r') as hf:\n",
    "        onehot_sequences = hf['onehot_sequences_bool'][:]\n",
    "    #onehot_sequences.shape\n",
    "    expdata = expressions\n",
    "    seqdata_transformed = onehot_sequences\n",
    "\n",
    "\n",
    "expressions = expdata\n",
    "onehot_sequences = seqdata_transformed\n",
    "\n",
    "\n",
    "## training, validation and test data split\n",
    "## Shuffling \n",
    "if 0:\n",
    "    randomize  =  np.random.permutation(len(onehot_sequences))\n",
    "    onehot_sequences = onehot_sequences[randomize,:]\n",
    "    expressions = expressions[randomize]\n",
    "\n",
    "\n",
    "expressions = np.reshape(expressions , [-1,1])\n",
    "\n",
    "fold_cv = 50\n",
    "total_seqs = len(onehot_sequences)\n",
    "_trX = onehot_sequences[int(total_seqs/fold_cv):]\n",
    "_trY = expressions[int(total_seqs/fold_cv):]\n",
    "\n",
    "_vaX = onehot_sequences[10000:int(total_seqs/fold_cv)]\n",
    "_vaY = expressions[10000:int(total_seqs/fold_cv)]\n",
    "\n",
    "\n",
    "_teX = onehot_sequences[0:10000]\n",
    "_teY = expressions[0:10000]\n",
    "\n",
    "\n",
    "_trX.shape , _trY.shape , _vaX.shape , _vaY.shape  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with h5py.File(os.path.join('..','..','data',model_conditions ,'_trX.h5'), 'w') as hf:\n",
    "    hf.create_dataset(\"_trX\",  data=_trX)  \n",
    "    \n",
    "with h5py.File(os.path.join('..','..','data',model_conditions ,'_trY.h5'), 'w') as hf:\n",
    "    hf.create_dataset(\"_trY\",  data=_trY)  \n",
    "\n",
    "with h5py.File(os.path.join('..','..','data',model_conditions ,'_vaX.h5'), 'w') as hf:\n",
    "    hf.create_dataset(\"_vaX\",  data=_vaX)  \n",
    "\n",
    "with h5py.File(os.path.join('..','..','data',model_conditions ,'_vaY.h5'), 'w') as hf:\n",
    "    hf.create_dataset(\"_vaY\",  data=_vaY)  \n",
    "\n",
    "with h5py.File(os.path.join('..','..','data',model_conditions ,'_teX.h5'), 'w') as hf:\n",
    "    hf.create_dataset(\"_teX\",  data=_teX)  \n",
    "\n",
    "with h5py.File(os.path.join('..','..','data',model_conditions ,'_teY.h5'), 'w') as hf:\n",
    "    hf.create_dataset(\"_teY\",  data=_teY)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate standard scaler for positive and negative selection experiments using the extreme cases of the designed sequences (Done together since designed sequences are from the same file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/SC_Ura/scaler.save']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthesized_seqs_filepath = os.path.join('..','..','data','synthesized_sequences_results',\n",
    "                                    '20190325_NBT_MolEvol_seq_data_SCUra_YPD.splitByOrigID.meanEL.min100reads.txt')\n",
    "\n",
    "def read_synthesized_sequences(filename) :\n",
    "\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        d = list(reader)\n",
    "    \n",
    "\n",
    "    scura_exp = [di[3] for di in d]\n",
    "    glu_exp = [di[4] for di in d]\n",
    "\n",
    "    ### first row is nonsense\n",
    "    scura_exp = scura_exp[1:]\n",
    "    glu_exp = glu_exp[1:]\n",
    "\n",
    "\n",
    "    return glu_exp,scura_exp\n",
    "\n",
    "glu_exp,scura_exp = read_synthesized_sequences(synthesized_seqs_filepath)\n",
    "\n",
    "def clean_exp(Y) :\n",
    "    exp_NA = [(a=='NA') for a in Y]\n",
    "    exp_NA = np.array(exp_NA)\n",
    "\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    clean_exp = Y[~exp_NA]\n",
    "    clean_exp = [float(a) for a in clean_exp ]\n",
    "    return clean_exp\n",
    "    \n",
    "clean_glu_exp = np.array(clean_exp(glu_exp)).reshape(-1, 1)\n",
    "clean_scura_exp = np.array(clean_exp(scura_exp)).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "#save Glu scaler \n",
    "glu_scaler = sklearn.preprocessing.StandardScaler()\n",
    "glu_scaler.fit(clean_glu_exp)\n",
    "sklearn.externals.joblib.dump(glu_scaler, os.path.join('..','..','data','Glu','scaler.save' ) )\n",
    "\n",
    "#save SC_Ura scaler \n",
    "scura_scaler = sklearn.preprocessing.StandardScaler()\n",
    "scura_scaler.fit(clean_scura_exp)\n",
    "sklearn.externals.joblib.dump(scura_scaler, os.path.join('..','..','data','SC_Ura','scaler.save' ) )\n",
    "\n",
    "#scaler = sklearn.externals.joblib.load(scaler_filename) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:me] *",
   "language": "python",
   "name": "conda-env-me-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
