{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the benchmarking models \n",
    "\n",
    "- To train the benchmarking models, simply run the following command on a terminal from within this folder: <br> <code>python benchmarking.py 'DeepSEA' </code> \n",
    "- If you would like to save the model, you can change the save flag to 1 in `line 16` of `benchmarking.py` \n",
    "- The code for benchmarking.py (which was used for training the benchmarking models) can also be inspected in the last section of this notebook. \n",
    "- Benchmarking.py trains a model from scratch, loads the random test data (corresponding to Fig. 1a), generates predictions and saves the model weights and the predictions for this data. \n",
    "- The resutls can also be generated from saved models using Notebook 1 from this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the ECC using the benchmarking models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the chosen benchmarking model ( DeepSEA / DeepAtt / DanQ )\n",
    "The saved model is available in this directory under the name `[model_name].h5`\n",
    "\n",
    "\n",
    "##### Note about the final layer :\n",
    "\n",
    "We emphasize that the final layer does have a <b>single output unit and a linear activation</b> as one would expect. This is can be verified by looking at the following line in the `benchmarking.py` file in this folder and also in the cell below : <br>\n",
    "<code>output_layer = Dense(1, kernel_regularizer = l1_l2(l1=l1_weight, l2=l2_weight),\n",
    "                     activation='linear', kernel_initializer='he_normal', use_bias=True )(x) \n",
    "</code>\n",
    "\n",
    "\n",
    "#####  Note about the DeepAtt() model : \n",
    "\n",
    "- The reader will notice that there is a fitness_function_model() call instead of DeepAtt() for the model_name = 'DeepAtt'. The reason is that there are some non JSON-serializable layers in the DeepAtt() function invocation (from their original authors) which makes it difficult to save using model.save(). The model can be trained (using notebook 2 in this folder) but the jupyter notebook needs to be kept running for the complete analysis (including computing predictions and computing ECC using this model) since model.save_weights() results in errors when loading and model.save() will not work. \n",
    "- The ECC for DeepAtt was also similarly calculated using this single running notebook from end to end using the DeepAtt() call to stay faithful to the original implementation. We also save a model trained using our own implementation of the DeepAtt() model with JSON-seriable layers for future predictions.\n",
    "- So, to save a version of the 'DeepAtt' model, we use our own layers to address the issues we had in saving with the implementation in DeepAtt() from the original authors. \n",
    "- It can be verified that both lead to equivalent performance by changing the 'define_DeepAtt_here' flag in Notebook 2 and the bechmarking.py file.\n",
    "- The DeepAtt() model was trained from scratch here again (in this same VM and environment) using both approaches (our adapted approach and the original authors approach) to verify the equivalent peformance and it worked as expected consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator StandardScaler from version 0.20.3 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'DeepSEA_model'#'DanQ_model' , 'DeepSEA_model' , 'DeepAtt_model'\n",
    "\n",
    "\n",
    "import BioinforDeepATT\n",
    "from BioinforDeepATT.model.model import *\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "from rr_aux import *\n",
    "\n",
    "\n",
    "##Clear Memory \n",
    "#tf.reset_default_graph()\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "if model_name in ['DanQ_model' , 'DeepSEA_model']:\n",
    "    model = tf.keras.models.load_model(model_name+'.h5')\n",
    "else :\n",
    "    \n",
    "\n",
    "    def fitness_function_model(model_params) :\n",
    "\n",
    "        batch_size= model_params['batch_size']\n",
    "        l1_weight= model_params['l1_weight']\n",
    "        l2_weight= model_params['l2_weight']\n",
    "        lr= model_params['lr']\n",
    "        device_type = model_params['device_type']\n",
    "        input_shape = model_params['input_shape']\n",
    "        loss = model_params['loss']\n",
    "        model_name = model_params['model_name']\n",
    "\n",
    "        if(model_params['device_type']=='tpu'):\n",
    "            input_layer = Input(batch_shape=(batch_size,input_shape[1],input_shape[2]))  #trX.shape[1:] #batch_shape=(batch_size,110,4)\n",
    "\n",
    "        else :\n",
    "            input_layer = Input(shape = input_shape[1:] , batch_size = 1024)  #trX.shape[1:] #\n",
    "\n",
    "        if model_name=='DeepAtt':\n",
    "            if 1 : \n",
    "                x = Conv1D(256, 30, padding='valid' ,\\\n",
    "                       kernel_regularizer  = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer='he_normal' ,\n",
    "                      data_format = 'channels_last' , activation='relu')(input_layer) \n",
    "                x = tf.keras.layers.MaxPool1D( pool_size=3, strides=3, padding='valid')(x)\n",
    "                x= tf.keras.layers.Dropout(0.2)(x)\n",
    "                x = Bidirectional(LSTM(16, return_sequences=True,kernel_initializer='he_normal'))(x)\n",
    "                x = MultiHeadAttention( head_num=16)(x) \n",
    "                x = tf.keras.layers.Dropout(0.2)(x)\n",
    "                x = Flatten()(x)\n",
    "\n",
    "                x = keras.layers.Dense(\n",
    "                        units=16,\n",
    "                        activation='relu')(x)\n",
    "                x = keras.layers.Dense(\n",
    "                    units=16,\n",
    "                    activation='relu')(x)\n",
    "            else : \n",
    "                x = DeepAtt()\n",
    "                x = x.call(input_layer)\n",
    "\n",
    "        if model_name=='DanQ':\n",
    "            x = DanQ()\n",
    "            x = x.call(input_layer)\n",
    "\n",
    "        if model_name=='DeepSEA':\n",
    "            x = DeepSEA()\n",
    "            x = x.call(input_layer)\n",
    "\n",
    "        if(len(x.get_shape())>2):\n",
    "            x = Flatten()(x) \n",
    "\n",
    "        output_layer = Dense(1, kernel_regularizer = l1_l2(l1=l1_weight, l2=l2_weight),\n",
    "                        activation='linear', kernel_initializer='he_normal', use_bias=True )(x) \n",
    "\n",
    "\n",
    "        model = Model(input_layer, output_layer)\n",
    "        opt = tf.compat.v1.train.AdamOptimizer(lr) #tf.keras.optimizers.Adam(lr=lr)#\n",
    "\n",
    "\n",
    "        model.compile(optimizer=opt, loss=loss,metrics=['mean_squared_error', 'cosine_similarity']) \n",
    "\n",
    "        return model\n",
    "\n",
    "    model_params = {\n",
    "            'n_val_epoch' : 1000,\n",
    "            'epochs' : 3, #3 used previously\n",
    "            'batch_size': int(1024*1), # int(1024*3) , #64*55 fits , #best batch size is 1024\n",
    "            'l1_weight': 0,#1e-6#1e-7#0.01 # l1 should always be zero\n",
    "            'l2_weight': 0,#1e-7#0.01\n",
    "            'lr':0.001,\n",
    "            'device_type' : 'gpu', #'tpu'/'gpu'/'cpu'\n",
    "            'input_shape' : (30722048, 110, 4),\n",
    "            'loss' : 'mean_squared_error', \n",
    "            'model_name' : 'DeepAtt'}\n",
    "\n",
    "    model=fitness_function_model(model_params)\n",
    "    model.load_weights('DeepAtt')\n",
    "scaler= sklearn.externals.joblib.load(os.path.join('..','..','..','data','Glu','scaler.save'))\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args  = {'sequence_length' : 80}\n",
    "np.random.seed(0)\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "### Thanks, Stack Overflow ! \n",
    "def mean_(val, freq):\n",
    "    return np.average(val, weights = freq)\n",
    "\n",
    "def median_(val, freq):\n",
    "    ord_ = np.argsort(val)\n",
    "    cdf = np.cumsum(freq[ord_])\n",
    "    return val[ord_][np.searchsorted(cdf[-1] // 2, cdf)]\n",
    "\n",
    "def mode_(val, freq): #in the strictest sense, assuming unique mode\n",
    "    return val[np.argmax(freq)]\n",
    "\n",
    "def var_(val, freq):\n",
    "    if(len(val)==1):\n",
    "        return 0\n",
    "    avg = mean_(val, freq)\n",
    "    dev = freq * (val - avg) ** 2\n",
    "    return dev.sum() / (freq.sum() - 1)\n",
    "\n",
    "def std_(val, freq):\n",
    "    return np.sqrt(var_(val, freq))\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "def freq_df(pro_ortho_sequences):\n",
    "    ### Returns : Base Frequency distribution for this set of sequences\n",
    "    #Verified against : http://hplgit.github.io/bioinf-py/doc/pub/html/main_bioinf.html\n",
    "    pro_freq_df = pd.DataFrame(index=range(len(pro_ortho_sequences[0])) , columns=['A','C','G','T'] ,\n",
    "                              data = np.zeros( (len(pro_ortho_sequences[0]),4)))\n",
    "    for sequence in pro_ortho_sequences : \n",
    "        for index,base in enumerate(sequence) : \n",
    "            pro_freq_df.loc[index,base]+=1\n",
    "            \n",
    "    pro_freq_df = pro_freq_df.div(pro_freq_df.sum(axis=1), axis=0)\n",
    "\n",
    "    return pro_freq_df\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "### Generate all possible single mutations in population : population_next \n",
    "\n",
    "\n",
    "def population_mutator( population_current , args) :\n",
    "    population_current = population_remove_flank(population_current)\n",
    "    population_next = []  \n",
    "    for i in range(len(population_current)) :         \n",
    "        for j in range(args['sequence_length']) : \n",
    "        #First create three copies of the same individual, one for each possible mutation at the basepair.\n",
    "            population_next.append(list(population_current[i]))\n",
    "            population_next.append(list(population_current[i]))\n",
    "            population_next.append(list(population_current[i]))\n",
    "            \n",
    "            if (population_current[i][j] == 'A') :\n",
    "                population_next[3*(args['sequence_length']*i + j) ][j] = 'C'\n",
    "                population_next[3*(args['sequence_length']*i + j) + 1][j] = 'G'\n",
    "                population_next[3*(args['sequence_length']*i + j) + 2][j] = 'T'\n",
    "                \n",
    "            elif (population_current[i][j] == 'C') :\n",
    "                population_next[3*(args['sequence_length']*i + j)][j] = 'A'\n",
    "                population_next[3*(args['sequence_length']*i + j) + 1][j] = 'G'\n",
    "                population_next[3*(args['sequence_length']*i + j) + 2][j] = 'T'\n",
    "            \n",
    "            elif (population_current[i][j] == 'G') :\n",
    "                population_next[3*(args['sequence_length']*i + j)][j] = 'C'\n",
    "                population_next[3*(args['sequence_length']*i + j) + 1][j] = 'A'\n",
    "                population_next[3*(args['sequence_length']*i + j) + 2][j] = 'T'\n",
    "                \n",
    "            elif (population_current[i][j] == 'T') :\n",
    "                population_next[3*(args['sequence_length']*i + j)][j] = 'C'\n",
    "                population_next[3*(args['sequence_length']*i + j) + 1][j] = 'G'\n",
    "                population_next[3*(args['sequence_length']*i + j) + 2][j] = 'A'\n",
    "             \n",
    "        \n",
    "    population_next= population_add_flank(population_next)        \n",
    "    return list(population_next)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "\n",
    "def closest_point(node, nodes):\n",
    "    nodes = np.asarray(nodes)\n",
    "    deltas = nodes - node\n",
    "    dist_2 = np.einsum('ij,ij->i', deltas, deltas)\n",
    "    return np.argmin(dist_2)\n",
    "\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "\n",
    "def get_snpdev_dist(population) : \n",
    "    population_fitness = np.array(evaluate_model(list(population),model,scaler,batch_size))\n",
    "    args  = {'sequence_length' : 80 , 'nucleotide_frequency' :[0.25,0.25,0.25,0.25] , 'randomizer' : np.random } \n",
    "    population_1bp_all_sequences = population_mutator(list(population) , args)\n",
    "    population_1bp_all_fitness = np.array(evaluate_model(list(population_1bp_all_sequences),model,scaler,batch_size))\n",
    "\n",
    "\n",
    "    snpdev_dist = []\n",
    "    for i in (range(len(population))) :   \n",
    "        original_fitness = population_fitness[i]\n",
    "        sequence = population[i]\n",
    "\n",
    "        exp_dist = population_1bp_all_fitness[3*args['sequence_length']*i:3*args['sequence_length']*(i+1)]\n",
    "        snpdev_dist = snpdev_dist + [np.sort((exp_dist-original_fitness))]\n",
    "\n",
    "    sequences = population\n",
    "    return snpdev_dist\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "def analyze_seq(population ) : \n",
    "    ### Quick function to generate an analyzed dataframe for any sequence. Only works in this NB.\n",
    "\n",
    "    data_at = AAnet_model.data2at(get_snpdev_dist(population))\n",
    "    Y_mds_data = data_at @ Y_mds_ats\n",
    "    \n",
    "    data_at_df = pd.DataFrame(data_at , columns = ['AT1','AT2','AT3']).join(\n",
    "    pd.DataFrame(Y_mds_data , columns = ['MDS1','MDS2'])).join(\n",
    "    pd.DataFrame(evaluate_model(population,model,scaler,batch_size),\n",
    "                 columns = [model_conditions+'_exp']))\n",
    "    return data_at_df, data_at , Y_mds_data\n",
    "\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "def snpdev_str_to_list(snpdev_str) : \n",
    "    return [float(i) for i in snpdev_str.replace(\"\\n\" , \"\").replace(\"[\",\"\").replace(\"]\",\"\").split()]\n",
    "\n",
    "def name2sys(gene_name):\n",
    "    return yeast_genome_annotations[yeast_genome_annotations.gene_name==gene_name].systematic_name.values[0]\n",
    "#########################################################################################################\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the ortholog sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ./rr_aux.py:240: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "62464/62464 [==============================] - 46s 730us/sample\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ortho_sequences_df = pd.read_csv('20190719_Native80.2_orthologous_promoters_unique.txt.gz', sep='\\t')\n",
    "ortho_sequences_df = ortho_sequences_df[~ortho_sequences_df.ortho_seq.str.contains('N')]\n",
    "nonunique_ortho_sequences_df = pd.read_csv('20190719_Native80.2_orthologous_promoters.txt.gz', sep='\\t')\n",
    "nonunique_ortho_sequences_df = nonunique_ortho_sequences_df[~nonunique_ortho_sequences_df.ortho_seq.str.contains('N')]\n",
    "### Compute Expression\n",
    "ortho_sequences_df['EL'] = evaluate_model(list(ortho_sequences_df.ortho_seq), \n",
    "                                                  model,scaler,batch_size)\n",
    "\n",
    "ortho_sequences = list(ortho_sequences_df['ortho_seq'].values)\n",
    "if len(ortho_sequences[0])==80 :\n",
    "    ortho_sequences = population_add_flank(list(ortho_sequences))\n",
    "\n",
    "### List of Unique Promoters\n",
    "proName_list = np.unique(ortho_sequences_df.proName.values)\n",
    "\n",
    "#! cp ../../figure_orthologs/pro_consensus_dict.npy .\n",
    "pro_consensus_dict = np.load('pro_consensus_dict.npy', allow_pickle=1).item()\n",
    "proName_list = list(pro_consensus_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the number of sequences at each hamming distance from in the 1011 orthologs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62464/62464 [==============================] - 44s 712us/sample\n"
     ]
    }
   ],
   "source": [
    "def find_k(i):\n",
    "    seq= nonunique_ortho_sequences_df.ortho_seq[i]\n",
    "    reference = pro_consensus_dict[nonunique_ortho_sequences_df.proName[i]] # pro_df.loc[nonunique_ortho_sequences_df.proName[i]].reference_sequence#\n",
    "    return int(hamming_distance(seq , reference))\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()-5\n",
    "pool = mp.Pool(num_cores)\n",
    "nonunique_ortho_sequences_df['k'] = pool.map(  find_k,  (nonunique_ortho_sequences_df.index))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "\n",
    "def find_k_unique(i):\n",
    "    seq= ortho_sequences_df.ortho_seq[i]\n",
    "    reference = pro_consensus_dict[ortho_sequences_df.proName[i]] #pro_df.loc[ortho_sequences_df.proName[i]].reference_sequence#\n",
    "    return int(hamming_distance(seq , reference))\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()-5\n",
    "pool = mp.Pool(num_cores)\n",
    "ortho_sequences_df['k'] = pool.map(  find_k_unique,  (ortho_sequences_df.index))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "ortho_sequences_df['EL'] = evaluate_model(list(ortho_sequences_df.ortho_seq), \n",
    "                                                  model,scaler,batch_size)\n",
    "nonunique_ortho_sequences_df['EL'] = nonunique_ortho_sequences_df.ortho_seq.map(\n",
    "    dict(zip(ortho_sequences_df.ortho_seq,ortho_sequences_df.EL)))\n",
    "#ortho_sequences_df.to_csv(model_name+'ortho_sequences_df_analyzed.tsv', sep='\\t')\n",
    "#nonunique_ortho_sequences_df.to_csv(model_name+'nonunique_ortho_sequences_df_analyzed.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5569/5569 [00:26<00:00, 210.02it/s]\n"
     ]
    }
   ],
   "source": [
    "pro_df = pd.DataFrame(index=proName_list , columns = {'var_EL_actual','var_EL_sim'})\n",
    "for i in tqdm(proName_list):\n",
    "    ortho_subset= ortho_sequences_df[ortho_sequences_df.proName==i]\n",
    "    pro_df.loc[i,'var_EL_actual'] = var_(ortho_subset.EL , ortho_subset.num_isolates)\n",
    "    #pro_df.loc[i,'var_EL_actual_unweighted'] = np.var(ortho_subset.EL)\n",
    "\n",
    "ref_df=pd.read_csv(os.path.join('..','..','..','data','native_sequences_only','allNativeChunks.all') , \n",
    "            header=None , sep='\\t',index_col=0 , names=['reference_sequence'])#.loc[pro_df.index].values\n",
    "\n",
    "\n",
    "pro_df = pro_df.join(ref_df)\n",
    "pro_stats = pd.read_csv('20190719_Native80.2_promoter_divergence.txt',\n",
    "                   sep='\\t' , index_col=0)\n",
    "pro_df = pro_df.join(pro_stats )\n",
    "\n",
    "#if 0 : \n",
    "#pro_df = pd.read_csv('./pro_df_analyzed.tsv',sep='\\t', index_col=0)\n",
    "\n",
    "proName_list  = pro_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the ECC \n",
    "`log_ratio` field in the table corresponds to the ECC.<br>\n",
    "For more detail on ECC calculation, please see: https://github.com/1edv/evolution/blob/master/manuscript_code/ecc_mr_fr/ecc_mr_fr.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643072/643072 [==============================] - 435s 676us/sample\n",
      "62464/62464 [==============================] - 42s 666us/sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5569/5569 [04:34<00:00, 20.28it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#! cp ../../figure_orthologs/sim_unique_df_full .\n",
    "sim_unique_df_full = pd.read_csv('./sim_unique_df_full',sep='\\t', index_col=0)\n",
    "#! cp ../../figure_orthologs/sim_unique_df_full_v2 .\n",
    "sim_unique_df_full_v2 = pd.read_csv('./sim_unique_df_full_v2',sep='\\t', index_col=0)\n",
    "\n",
    "sim_unique_df_full['EL'] = evaluate_model(list(sim_unique_df_full.sim_seq), model,scaler,batch_size)\n",
    "sim_unique_df_full_v2['EL'] = evaluate_model(list(sim_unique_df_full_v2.sim_seq), model,scaler,batch_size)\n",
    "\n",
    "\n",
    "for pro in tqdm(proName_list):\n",
    "    sim_subset= sim_unique_df_full[sim_unique_df_full.proName==pro]\n",
    "    pro_df.loc[pro,'std_EL_sim'] = std_(sim_subset.EL , sim_subset.num_isolates) #STD not var, ignore name\n",
    "    pro_df.loc[pro,'var_EL_sim'] = var_(sim_subset.EL , sim_subset.num_isolates) #STD not var, ignore name\n",
    "\n",
    "    sim_subset_v2= sim_unique_df_full_v2[sim_unique_df_full_v2.proName==pro]\n",
    "    pro_df.loc[pro,'std_EL_sim_v2'] = std_(sim_subset_v2.EL , sim_subset_v2.num_isolates)#STD not var, ignore name\n",
    "    pro_df.loc[pro,'var_EL_sim_v2'] = var_(sim_subset_v2.EL , sim_subset_v2.num_isolates)#STD not var, ignore name\n",
    "\n",
    "    ortho_subset= ortho_sequences_df[ortho_sequences_df.proName==pro]\n",
    "    pro_df.loc[pro,'std_EL_actual'] = std_(ortho_subset.EL , ortho_subset.num_isolates)#STD not var, ignore name\n",
    "    pro_df.loc[pro,'var_EL_actual'] = var_(ortho_subset.EL , ortho_subset.num_isolates)#STD not var, ignore name\n",
    "\n",
    "\n",
    "pro_df['uncorrected_ratio'] =  (pro_df.std_EL_sim+(np.finfo(float).eps)) / (pro_df.std_EL_actual +(np.finfo(float).eps))\n",
    "pro_df['uncorrected_log_ratio'] = pro_df.uncorrected_ratio.astype('float').apply(np.log2)\n",
    "\n",
    "pro_df['ratio_v2'] =  (pro_df.std_EL_sim_v2+(np.finfo(float).eps)) / (pro_df.std_EL_actual +(np.finfo(float).eps))\n",
    "pro_df['log_ratio_v2'] = pro_df.ratio_v2.astype('float').apply(np.log2)\n",
    "\n",
    "pro_consensus_dict_df = pd.DataFrame.from_dict(pro_consensus_dict , orient='index')\n",
    "pro_consensus_dict_df.columns = ['consensus_sequence']\n",
    "pro_df = pro_df.join(pro_consensus_dict_df)\n",
    "\n",
    "pro_ref_pred_EL_df = pd.read_csv(os.path.join('..','..','..','data','native_sequences_only','pro_ref_pred_SC_URA_GLU_EL.tsv'), sep='\\t' ,\n",
    "               index_col = 0 )\n",
    "pro_df = pro_df.join(pro_ref_pred_EL_df.loc[pro_df.index , ['pred_ref_SC_Ura_EL' , 'pred_ref_Glu_EL']])\n",
    "\n",
    "pro_df.loc[pro_df.var_EL_actual==0,'uncorrected_log_ratio']=np.nan\n",
    "pro_df.loc[pro_df.var_EL_actual==0,'uncorrected_ratio']=np.nan\n",
    "\n",
    "pro_df.loc[pro_df.var_EL_actual==0,'log_ratio']=np.nan\n",
    "pro_df.loc[pro_df.var_EL_actual==0,'ratio']=np.nan\n",
    "\n",
    "pro_df.loc[pro_df.var_EL_actual==0,'log_ratio_v2']=np.nan\n",
    "pro_df.loc[pro_df.var_EL_actual==0,'ratio_v2']=np.nan\n",
    "\n",
    "\n",
    "pro_df['ratio_sim_sim'] =  (pro_df.std_EL_sim+(np.finfo(float).eps)) / (pro_df.std_EL_sim_v2 +(np.finfo(float).eps))\n",
    "pro_df['log_ratio_sim_sim'] = pro_df.ratio_sim_sim.astype('float').apply(np.log2)\n",
    "sim_sim_median = np.median(pro_df['log_ratio_sim_sim'])\n",
    "\n",
    "pro_df['log_ratio'] = pro_df['uncorrected_log_ratio'] - sim_sim_median\n",
    "\n",
    "\n",
    "if 0 : ### change to 1 if you would like to save the ECC results using the alternative model\n",
    "\n",
    "    pro_df.to_csv(model_name+'_pro_df_analyzed.tsv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code block computes mutation tolerance\n",
    "if 0 : \n",
    "    cons_df = pro_df[pro_df.log_ratio>0]\n",
    "    epsilon = 2*std_(cons_df.var_EL_actual.values.astype(np.float64) ,cons_df.num_valid_hamming.values )\n",
    "    print('Epsilon : ' + str(epsilon))\n",
    "    reference_snpdev_dist = get_snpdev_dist(population_add_flank(list(pro_df.reference_sequence)))\n",
    "    consensus_snpdev_dist = get_snpdev_dist(population_add_flank(list(pro_df.consensus_sequence)))\n",
    "\n",
    "    mutation_tolerance = ((np.abs(consensus_snpdev_dist) -epsilon)<0).sum(axis=1) / len(consensus_snpdev_dist[0])\n",
    "    pro_df['mutation_tolerance']=mutation_tolerance\n",
    "\n",
    "    if 0 : ### change to 1 if you would like to save the ECC results using the alternative model\n",
    "        pro_df.to_csv(model_name+'_pro_df_analyzed.tsv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for training the benchmarking models \n",
    "\n",
    "#### benchmarking.py was used to run the following code blocks , the cells below are just for convenient reading\n",
    "#### Benchmarking.py trains a model, loads the random test data (corresponding to Fig. 1a), generates predictions and saves the model weights and the predictions for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BioinforDeepATT\n",
    "from BioinforDeepATT.model.model import *\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from rr_aux import *\n",
    "\n",
    "\n",
    "##Clear Memory \n",
    "tf.reset_default_graph()\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "##\n",
    "\n",
    "define_DeepAtt_here = 1 \n",
    "save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NUM_GPU = len(get_available_gpus())\n",
    "if(NUM_GPU>0):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "model_conditions = 'Glu'\n",
    "\n",
    "\n",
    "#sys.argv[1] appended to beginning of path\n",
    "dir_path=os.path.join('..','..','..','data',model_conditions)\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Load the data matrix\n",
    "with h5py.File(join(dir_path,'_trX.h5'), 'r') as hf:\n",
    "    _trX = hf['_trX'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_trY.h5'), 'r') as hf:\n",
    "    _trY = hf['_trY'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_vaX.h5'), 'r') as hf:\n",
    "    _vaX = hf['_vaX'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_vaY.h5'), 'r') as hf:\n",
    "    _vaY = hf['_vaY'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_teX.h5'), 'r') as hf:\n",
    "    _teX = hf['_teX'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_teY.h5'), 'r') as hf:\n",
    "    _teY = hf['_teY'][:]\n",
    "\n",
    "\n",
    "\n",
    "_trX.shape , _trY.shape , _vaX.shape , _vaY.shape  , _teX.shape , _teY.shape\n",
    "\n",
    "\n",
    "\n",
    "trX = _trX[:int(_trX.shape[0]/1024)*1024] #np.concatenate((_trX, _trX_rc), axis = 1) #np.squeeze((_trX))#\n",
    "vaX = _vaX[:int(_vaX.shape[0]/1024)*1024] # np.concatenate((_vaX, _vaX_rc), axis = 1) #np.squeeze((_vaX))#\n",
    "teX = _teX[:int(_teX.shape[0]/1024)*1024] # np.concatenate((_teX, _teX_rc), axis = 1)#np.squeeze((_teX))#\n",
    "\n",
    "\n",
    "\n",
    "## Load the scaler function (scaler was trained on the synthesized data\n",
    "scaler = sklearn.externals.joblib.load(join(dir_path,'scaler.save')) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vaY = (scaler.transform(_vaY.reshape(1, -1))).reshape(_vaY.shape)[:int(_vaY.shape[0]/1024)*1024] #_vaY#\n",
    "trY = (scaler.transform(_trY.reshape(1, -1))).reshape(_trY.shape)[:int(_trY.shape[0]/1024)*1024] #_trY#\n",
    "teY = (scaler.transform(_teY.reshape(1, -1))).reshape(_teY.shape)[:int(_teY.shape[0]/1024)*1024] #_teY#\n",
    "\n",
    "\n",
    "### If using generator, have a smaller val set for faster evaluation\n",
    "if 0: \n",
    "    s_trX = np.vstack((trX , vaX))\n",
    "    s_trY = np.vstack((trY , vaY))\n",
    "    trX = s_trX[1000:,:]\n",
    "    trY = s_trY[1000:,:]\n",
    "    vaX = s_trX[0:1000,:]\n",
    "    vaY = s_trY[0:1000,:]\n",
    "\n",
    "print(trX.shape , trY.shape , vaX.shape , vaY.shape  , _teX.shape , _teY.shape)\n",
    "\n",
    "input_shape = trX.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BioinforDeepATT\n",
    "from BioinforDeepATT.model.model import *\n",
    "if define_DeepAtt_here == 1 :\n",
    "    del Bidirectional, LSTM\n",
    "from rr_aux import *  ### Overrides DeepAtt() imports\n",
    "\n",
    "def fitness_function_model(model_params) :\n",
    "\n",
    "    batch_size= model_params['batch_size']\n",
    "    l1_weight= model_params['l1_weight']\n",
    "    l2_weight= model_params['l2_weight']\n",
    "    lr= model_params['lr']\n",
    "    device_type = model_params['device_type']\n",
    "    input_shape = model_params['input_shape']\n",
    "    loss = model_params['loss']\n",
    "    model_name = model_params['model_name']\n",
    "    \n",
    "    if(model_params['device_type']=='tpu'):\n",
    "        input_layer = Input(batch_shape=(batch_size,input_shape[1],input_shape[2]))  #trX.shape[1:] #batch_shape=(batch_size,110,4)\n",
    "\n",
    "    else :\n",
    "        input_layer = Input(shape = input_shape[1:] , batch_size = 1024)  #trX.shape[1:] #\n",
    "\n",
    "    if model_name=='DeepAtt':\n",
    "        if define_DeepAtt_here : \n",
    "            x = Conv1D(256, 30, padding='valid' ,\\\n",
    "                   kernel_regularizer  = l1_l2(l1=l1_weight, l2=l2_weight), kernel_initializer='he_normal' ,\n",
    "                  data_format = 'channels_last' , activation='relu')(input_layer) \n",
    "            x = tf.keras.layers.MaxPool1D( pool_size=3, strides=3, padding='valid')(x)\n",
    "            x= tf.keras.layers.Dropout(0.2)(x)\n",
    "            x = Bidirectional(LSTM(16, return_sequences=True,kernel_initializer='he_normal'))(x)\n",
    "            x = MultiHeadAttention( head_num=16)(x) \n",
    "            x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            x = Flatten()(x)\n",
    "\n",
    "            x = keras.layers.Dense(\n",
    "                    units=16,\n",
    "                    activation='relu')(x)\n",
    "            x = keras.layers.Dense(\n",
    "                units=16,\n",
    "                activation='relu')(x)\n",
    "        else : \n",
    "            x = DeepAtt()\n",
    "            x = x.call(input_layer)\n",
    "\n",
    "    if model_name=='DanQ':\n",
    "        x = DanQ()\n",
    "        x = x.call(input_layer)\n",
    "\n",
    "    if model_name=='DeepSEA':\n",
    "        x = DeepSEA()\n",
    "        x = x.call(input_layer)\n",
    "\n",
    "    if(len(x.get_shape())>2):\n",
    "        x = Flatten()(x) \n",
    "        \n",
    "    output_layer = Dense(1, kernel_regularizer = l1_l2(l1=l1_weight, l2=l2_weight),\n",
    "                    activation='linear', kernel_initializer='he_normal', use_bias=True )(x) \n",
    "\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    opt = tf.compat.v1.train.AdamOptimizer(lr) #tf.keras.optimizers.Adam(lr=lr)#\n",
    "    \n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss,metrics=['mean_squared_error', 'cosine_similarity']) \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name_list = sys.argv#['DeepSEA' , 'DanQ' , 'DeepAtt'  ]\n",
    "model_name = 'DeepSEA'#model_name_list[0]\n",
    "model_params = {\n",
    "    'n_val_epoch' : 1000,\n",
    "    'epochs' : 3,\n",
    "    'batch_size': int(1024*1), # int(1024*3) , #64*55 fits , #best batch size is 1024\n",
    "    'l1_weight': 0,#1e-6#1e-7#0.01 # l1 should always be zero\n",
    "    'l2_weight': 0,#1e-7#0.01\n",
    "    'lr':0.001,\n",
    "    'device_type' : 'gpu', #'tpu'/'gpu'/'cpu'\n",
    "    'input_shape' : input_shape,#input_shape,\n",
    "    'loss' : 'mean_squared_error', \n",
    "    'model_name' : model_name}\n",
    "epochs = model_params['epochs']  \n",
    "batch_size =  model_params['batch_size']\n",
    "n_val_epoch =  model_params['n_val_epoch']\n",
    "epochs = model_params['epochs']\n",
    "\n",
    "\n",
    "\n",
    "if 0 : \n",
    "    ### Save model params as csv\n",
    "    w = csv.writer(open(os.path.join(model_name+'_model_params.csv'), \"w\"))\n",
    "    for key, val in model_params.items():\n",
    "        w.writerow([key, val])\n",
    "\n",
    "    ### Save model params as pickle\n",
    "    f = open(os.path.join(model_name+'_model_params.pkl'),\"wb\")\n",
    "    pickle.dump(model_params,f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "model=fitness_function_model(model_params)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(trX, trY, validation_data = (vaX[:1024], vaY[:1024]), batch_size=batch_size  , epochs=epochs  )\n",
    "#model.fit_generator(training_generator, validation_data = (teX[:100], teY[:100]),\n",
    "#epochs=epochs , steps_per_epoch = int(trX.shape[0]/(batch_size*n_val_epoch)) )\n",
    "\n",
    "\n",
    "\n",
    "def read_hq_testdata(filename) :\n",
    "\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        d = list(reader)\n",
    "\n",
    "    sequences = [di[0] for di in d]\n",
    "\n",
    "    for i in tqdm(range(0,len(sequences))) : \n",
    "        if (len(sequences[i]) > 110) :\n",
    "            sequences[i] = sequences[i][-110:]\n",
    "        if (len(sequences[i]) < 110) : \n",
    "            while (len(sequences[i]) < 110) :\n",
    "                sequences[i] = 'N'+sequences[i]\n",
    "\n",
    "\n",
    "\n",
    "    A_onehot = np.array([1,0,0,0] ,  dtype=np.bool)\n",
    "    C_onehot = np.array([0,1,0,0] ,  dtype=np.bool)\n",
    "    G_onehot = np.array([0,0,1,0] ,  dtype=np.bool)\n",
    "    T_onehot = np.array([0,0,0,1] ,  dtype=np.bool)\n",
    "    N_onehot = np.array([0,0,0,0] ,  dtype=np.bool)\n",
    "\n",
    "    mapper = {'A':A_onehot,'C':C_onehot,'G':G_onehot,'T':T_onehot,'N':N_onehot}\n",
    "    worddim = len(mapper['A'])\n",
    "    seqdata = np.asarray(sequences)\n",
    "    seqdata_transformed = seq2feature(seqdata)\n",
    "    print(seqdata_transformed.shape)\n",
    "\n",
    "\n",
    "    expressions = [di[1] for di in d]\n",
    "    expdata = np.asarray(expressions)\n",
    "    expdata = expdata.astype('float')  \n",
    "\n",
    "    return np.squeeze(seqdata_transformed),expdata\n",
    "\n",
    "X,Y = read_hq_testdata(os.path.join('..','..','..','data','Glu','HQ_testdata.txt'))\n",
    "Y = [float(x) for x in Y]\n",
    "Y_pred= evaluate_model(X, model, scaler)\n",
    "\n",
    "pcc = scipy.stats.pearsonr(Y,Y_pred )[0]\n",
    "print(pcc)\n",
    "\n",
    "if save : \n",
    "    df = pd.DataFrame({'Y' : Y , 'Y_pred' : Y_pred , 'pcc' : pcc})\n",
    "    df.to_csv(model_name+\"_results.csv\")\n",
    "    model.save(model_name+\"_model.h5\")\n",
    "    model.save_weights(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:evolution] *",
   "language": "python",
   "name": "conda-env-evolution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
