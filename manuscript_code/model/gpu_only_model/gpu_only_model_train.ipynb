{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains code for training the version of the model that runs on only on the GPUs (and is incompatible with TPUs).\n",
    "The conda environment required for running this notebook can be installed and activated by running the following on the command line from within this folder: \n",
    "<code>conda env create -f evolution_env_gpu_only.yml </code>  \\\n",
    "<code>conda activate seq</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse,pwd,os,numpy as np,h5py\n",
    "from os.path import splitext,exists,dirname,join,basename\n",
    "from os import makedirs\n",
    "from itertools import izip\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import tensorflow as tf, sys, numpy as np, h5py, pandas as pd\n",
    "from tensorflow import nn\n",
    "from tensorflow.contrib import rnn\n",
    "from os.path import join,dirname,basename,exists,realpath\n",
    "from os import makedirs\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import sklearn , scipy\n",
    "from sklearn.metrics import *\n",
    "from scipy.stats import *\n",
    "import time\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='Input model conditions')\n",
    "#parser.add_argument('-m','--model_conditions',  help='Model Conditions', required=True)\n",
    "#args = vars(parser.parse_args())\n",
    "args['model_conditions'] = 'pTpA_' \n",
    "\n",
    "model_conditions = args['model_conditions']\n",
    "\n",
    "\n",
    "## Load the data matrix \n",
    "with h5py.File(join('..',model_conditions,'expression.h5'), 'r') as hf: #replace with path to your data\n",
    "    expressions = hf['expression'][:]\n",
    "#expressions.shape\n",
    "\n",
    "## Load the sequences \n",
    "with h5py.File(join('..',model_conditions,'onehot_sequences_bool.h5'), 'r') as hf:#replace with path to your data\n",
    "    onehot_sequences = hf['onehot_sequences_bool'][:]\n",
    "#onehot_sequences.shape\n",
    "\n",
    "## Load the rc sequences \n",
    "with h5py.File(join('..',model_conditions,'rc_onehot_sequences_bool.h5'), 'r') as hf:#replace with path to your data\n",
    "    rc_onehot_sequences = hf['rc_onehot_sequences_bool'][:]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "## training, validation and test data split\n",
    "## Shuffling \n",
    "randomize  =  np.random.permutation(len(onehot_sequences))\n",
    "onehot_sequences = onehot_sequences[randomize,:]\n",
    "rc_onehot_sequences = rc_onehot_sequences[randomize,:]\n",
    "expressions = expressions[randomize]\n",
    "expressions = np.reshape(expressions , [-1,1])\n",
    "\n",
    "total_seqs = len(onehot_sequences)\n",
    "_trX = onehot_sequences[int(total_seqs/10):]\n",
    "_trX_rc = rc_onehot_sequences[int(total_seqs/10):]\n",
    "_trY = expressions[int(total_seqs/10):]\n",
    "\n",
    "_vaX = onehot_sequences[0:int(total_seqs/10)]\n",
    "_vaX_rc = rc_onehot_sequences[0:int(total_seqs/10)]\n",
    "_vaY = expressions[0:int(total_seqs/10)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_trX.shape , _trY.shape , _vaX.shape , _vaY.shape  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##MODEL FILE SAVING ADDRESSES AND MINIBATCH SIZES\n",
    "_batch_size = 1024\n",
    "_hyper_train_size = 2000\n",
    "#_valid_size = 1024\n",
    "_hidden = 256\n",
    "_epochs = 15\n",
    "_best_model_file = join('..',model_conditions,'models','best_model.ckpt')\n",
    "_best_model_file_hyper = join('..',model_conditions,'models','hyper_search', 'best_model.ckpt')\n",
    "for _file  in [_best_model_file, _best_model_file_hyper]:\n",
    "    if not exists(dirname(_file)):\n",
    "        makedirs(dirname(_file))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "### MODEL ARCHITECTURE\n",
    "####################################################################\n",
    "####################################################################\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride=1):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x, stride=2, filter_size=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 2, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "def cross_entropy(y, y_real):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y, labels = y_real))\n",
    "\n",
    "def build_two_fc_layers(x_inp, Ws, bs):\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_inp, Ws[0]) + bs[0])\n",
    "    return tf.matmul(h_fc1, Ws[1]) + bs[1]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "def cnn_model(X, hyper_params , scope):\n",
    "\n",
    "    with tf.variable_scope(scope) : \n",
    "        global _hidden \n",
    "        conv1_filter_dim1 = 30\n",
    "        conv1_filter_dim2 = 4\n",
    "        conv1_depth = _hidden\n",
    "\n",
    "        conv2_filter_dim1 = 30\n",
    "        conv2_filter_dim2 = 1\n",
    "        conv2_depth = _hidden\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        W_conv1 = weight_variable([1,conv1_filter_dim1,conv1_filter_dim2,conv1_depth])\n",
    "        conv1 = conv2d(X, W_conv1)    \n",
    "        conv1 = tf.nn.bias_add(conv1, bias_variable([conv1_depth]))\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        l_conv = conv1\n",
    "        \n",
    "        W_conv2 = weight_variable([conv2_filter_dim1,conv2_filter_dim2,conv1_depth, conv2_depth])\n",
    "        conv2 = conv2d(conv1,W_conv2 )\n",
    "        conv2 = tf.nn.bias_add(conv2, bias_variable([conv2_depth]))\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "        \n",
    "        regularization_term = hyper_params['l2']* tf.reduce_mean(tf.abs(W_conv1)) + hyper_params['l2']* tf.reduce_mean(tf.abs(W_conv2)) \n",
    "        \n",
    "        cnn_model_output = conv2\n",
    "\n",
    "    return cnn_model_output , regularization_term \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def training(trX, trX_rc, trY, valX, valX_rc, valY, hyper_params, epochs, batch_size, best_model_file): \n",
    "\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    global _hidden \n",
    "\n",
    "    lstm_num_hidden = _hidden\n",
    "    fc_num_hidden = _hidden\n",
    "    num_classes = 1\n",
    "    num_bins = 256\n",
    "    \n",
    "    conv3_filter_dim1 = 30\n",
    "    conv3_filter_dim2 = 1\n",
    "    conv3_depth = _hidden\n",
    "    \n",
    "    conv4_filter_dim1 = 30\n",
    "    conv4_filter_dim2 = 1\n",
    "    conv4_depth = _hidden\n",
    "        # Input and output\n",
    "\n",
    "    X = tf.placeholder(\"float\", [None, 1, 110, 4] )\n",
    "    X_rc = tf.placeholder(\"float\", [None, 1, 110, 4] )\n",
    "    Y = tf.placeholder(\"float\", [None,1] )\n",
    "    dropout_keep_probability = tf.placeholder_with_default(1.0, shape=())\n",
    "\n",
    "\n",
    "    #f is forward sequence \n",
    "    output_f , regularization_term_f =  cnn_model(X, {'dropout_keep':hyper_params['dropout_keep'],'l2':hyper_params['l2']} , \"f\")\n",
    "\n",
    "    #rc is reverse complement of that sequence\n",
    "    output_rc , regularization_term_rc =  cnn_model(X_rc, {'dropout_keep':hyper_params['dropout_keep'],'l2':hyper_params['l2']} , \"rc\")\n",
    "    \n",
    "    \n",
    "    ### CONCATENATE output_f and output_rc\n",
    "    concatenated_f_rc = tf.concat([output_f , output_rc], -1)\n",
    "    ###\n",
    "    \n",
    "    W_conv3 = weight_variable([conv3_filter_dim1,conv3_filter_dim2,2*_hidden,conv3_depth])\n",
    "    conv3 = conv2d(concatenated_f_rc,W_conv3 )\n",
    "    conv3 = tf.nn.bias_add(conv3, bias_variable([conv3_depth]))\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "    W_conv4 = weight_variable([conv4_filter_dim1,conv4_filter_dim2,conv3_depth,conv4_depth])\n",
    "    conv4 = conv2d(conv3,W_conv4 )\n",
    "    conv4 = tf.nn.bias_add(conv4, bias_variable([conv4_depth]))\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "\n",
    "\n",
    "    conv_feat_map_x = 110   \n",
    "    conv_feat_map_y =  1   \n",
    "    h_conv_flat = tf.reshape(conv4, [-1, conv_feat_map_x * conv_feat_map_y * lstm_num_hidden])\n",
    "    #FC-1\n",
    "    \n",
    "    W_fc1 = weight_variable([conv_feat_map_x * conv_feat_map_y * lstm_num_hidden , fc_num_hidden])\n",
    "    b_fc1 = bias_variable([fc_num_hidden])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv_flat, W_fc1) + b_fc1)\n",
    "    #Dropout for FC-1\n",
    "    h_fc1 = tf.nn.dropout(h_fc1, dropout_keep_probability)\n",
    "\n",
    "    \n",
    "    #FC-2\n",
    "    W_fc2 = weight_variable([fc_num_hidden , num_bins])\n",
    "    b_fc2 = bias_variable([num_bins])\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    #Dropout for FC-2\n",
    "    h_fc2 = tf.nn.dropout(h_fc2, dropout_keep_probability)\n",
    "\n",
    "\n",
    "    #FC-3\n",
    "    W_fc3 = weight_variable([num_bins, num_classes])\n",
    "    b_fc3 = bias_variable([num_classes])\n",
    "    h_fc3 = tf.matmul(h_fc2, W_fc3) + b_fc3 \n",
    "\n",
    "    regularization_term = hyper_params['l2']* tf.reduce_mean(tf.abs(W_fc3)) + hyper_params['l2']* tf.reduce_mean(tf.abs(W_fc2)) + hyper_params['l2']* tf.reduce_mean(tf.abs(W_fc1)) + hyper_params['l2']* tf.reduce_mean(tf.abs(W_conv3))+ regularization_term_f + regularization_term_rc +hyper_params['l2']* tf.reduce_mean(tf.abs(W_conv4))\n",
    "\n",
    "\n",
    "\n",
    "    with tf.variable_scope(\"out\") :\n",
    "        output = h_fc3\n",
    "        model_output = tf.identity(output, name=\"model_output\")\n",
    "\n",
    "        ##########\n",
    "\n",
    "        loss = tf.losses.mean_squared_error( Y , model_output ) + regularization_term\n",
    "        cost = loss \n",
    "        model_cost = tf.identity(cost, name=\"model_cost\")\n",
    "        ##########\n",
    "        pcc = tf.contrib.metrics.streaming_pearson_correlation(model_output,Y)\n",
    "        model_pcc = tf.identity(pcc, name=\"model_pcc\")\n",
    "        ##########\n",
    "        mse = tf.losses.mean_squared_error( Y , model_output )\n",
    "        model_mse = tf.identity(mse, name=\"model_mse\")\n",
    "        ##########\n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(Y, tf.reduce_mean(Y))))\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(Y, model_output)))\n",
    "        R_squared = tf.subtract(tf.constant(\n",
    "    1,\n",
    "    dtype=tf.float32), tf.div(unexplained_error, total_error))\n",
    "        model_R_squared = tf.identity(R_squared, name=\"model_R_squared\")\n",
    "\n",
    "        ##########\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    tf.summary.scalar(\"cost\", model_cost)\n",
    "    tf.summary.scalar(\"pcc\", model_pcc[0])\n",
    "    tf.summary.scalar(\"mse\", model_mse)\n",
    "    tf.summary.scalar(\"R_squared\", R_squared)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(hyper_params['lr']).minimize(cost)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    best_cost = float(\"inf\") \n",
    "    best_r2 = float(0) \n",
    "\n",
    "    \n",
    "    batches_per_epoch = int(len(trX)/batch_size)\n",
    "    num_steps = int(epochs * batches_per_epoch)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init=tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    #init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    #clear the logs directory\n",
    "\n",
    "    now = datetime.now()\n",
    "    writer = tf.summary.FileWriter(join('..',model_conditions,'logs' , now.strftime(\"%Y%m%d-%H%M%S\") ), sess.graph)\n",
    "\n",
    "    print 'Initializing variables...'\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_pcc = 0\n",
    "    epoch_mse = 0\n",
    "    epoch_R_squared = 0\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    for step in tqdm(xrange(num_steps)):\n",
    "    \n",
    "    \n",
    "    \n",
    "        offset = (step * batch_size) % (trX.shape[0] - batch_size)\n",
    "        batch_x = trX[offset:(offset + batch_size), :]\n",
    "        batch_x_rc = trX_rc[offset:(offset + batch_size), :]\n",
    "        batch_y = trY[offset:(offset + batch_size)]\n",
    "\n",
    "        \n",
    "        feed_dict = {X: batch_x, X_rc: batch_x_rc ,  Y: batch_y , dropout_keep_probability : hyper_params['dropout_keep'] }\n",
    "        \n",
    "        _, batch_loss , batch_pcc , batch_mse, batch_R_squared , summary = sess.run([train_op, cost , pcc , mse, R_squared , summary_op], feed_dict=feed_dict)\n",
    "        \n",
    "        batch_R_squared = batch_pcc[0]**2\n",
    "        epoch_loss += batch_loss\n",
    "        epoch_pcc += batch_pcc[0]\n",
    "        epoch_mse += batch_mse        \n",
    "        epoch_R_squared += batch_R_squared    \n",
    "        \n",
    "        writer.add_summary(summary, step)\n",
    "\n",
    "        \n",
    "        \n",
    "        if ( (step % batches_per_epoch == 0) and step/batches_per_epoch!=0):\n",
    "\n",
    "            epoch_loss /= batches_per_epoch\n",
    "            epoch_pcc /= batches_per_epoch\n",
    "            epoch_mse /= batches_per_epoch\n",
    "            epoch_R_squared /= batches_per_epoch\n",
    "            \n",
    "            print ''\n",
    "            print ''\n",
    "            print ''\n",
    "            print ''\n",
    "            print 'Training - Avg batch loss at epoch %d: %f' % (step/batches_per_epoch, epoch_loss)\n",
    "            print 'Training - PCC : %f' % epoch_pcc\n",
    "            print 'Training - MSE : %f' % epoch_mse\n",
    "            print 'Training - R_squared : %f' % epoch_pcc**2\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            epoch_pcc = 0\n",
    "            epoch_mse = 0\n",
    "            epoch_R_squared = 0  \n",
    "            \n",
    "            #Randomized validation subset start\n",
    "            randomize  =  np.random.permutation(len(valX))\n",
    "            vaX = valX[randomize,:]\n",
    "            vaX_rc = valX_rc[randomize,:]\n",
    "            vaY = valY[randomize,:]\n",
    "            \n",
    "            #valX = vaX[0:valid_size,:] \n",
    "            #valX_rc = vaX_rc[0:valid_size,:] \n",
    "            #valY = vaY[0:valid_size,:]            \n",
    "            \n",
    "            #with tf.device('/cpu:0'):\n",
    "            #validation_cost , validation_acc , summary = sess.run([cost , accuracy , summary_op], feed_dict={X: valX , X_rc: valX_rc, Y: valY})\n",
    "            \n",
    "            #### teX_output contains TESTED SEQUENCES FOR VALIDATION SET\n",
    "            va_batch_size = 1024\n",
    "            (q,r) = divmod(vaX.shape[0] , va_batch_size)\n",
    "            i=0\n",
    "            vaX_output = []\n",
    "            while(i <= q ) :\n",
    "                if(i< q  ) :\n",
    "                    temp_result_step1=sess.run([model_output], feed_dict={X: vaX[va_batch_size*i:va_batch_size*i+va_batch_size,:], X_rc: vaX_rc[va_batch_size*i:va_batch_size*i+va_batch_size,:]  ,Y: vaY[va_batch_size*i:va_batch_size*i+va_batch_size,:]}) \n",
    "                    temp_result_step2=[float(x) for x in temp_result_step1[0]]\n",
    "                    #print temp_result_step2\n",
    "\n",
    "                    vaX_output = vaX_output + temp_result_step2\n",
    "                    i = i+1\n",
    "\n",
    "                elif (i==q) :\n",
    "                    temp_result_step1 = sess.run([model_output], feed_dict={X: vaX[va_batch_size*i:,:], X_rc: vaX_rc[va_batch_size*i:,:]  ,Y: vaY[va_batch_size*i:,:]})\n",
    "                    temp_result_step2=[float(x) for x in temp_result_step1[0]]\n",
    "                    #print \"here\"\n",
    "                    vaX_output = vaX_output + temp_result_step2\n",
    "                    i = i+1\n",
    "\n",
    "            #### RETURN TESTED SEQUENCES FOR VALIDATION SET\n",
    "            vaY = [float(x) for x in vaY]\n",
    "            validation_mse = sklearn.metrics.mean_squared_error(vaY , vaX_output )\n",
    "\n",
    "            validation_pcc = scipy.stats.pearsonr(vaY , vaX_output )\n",
    "            validation_r2  = validation_pcc[0]**2\n",
    "        \n",
    "        \n",
    "        \n",
    "            #for tensorboard\n",
    "            print ''\n",
    "            print 'Full Validation Set - MSE : %f' % validation_mse\n",
    "            print 'Full Validation Set - PCC : %f' % validation_pcc[0]\n",
    "            print 'Full Validation Set - R_squared : %f' % validation_r2\n",
    "\n",
    "            if(best_r2 < validation_r2) :\n",
    "                #\n",
    "                \n",
    "                #SAVER \n",
    "                saver = tf.train.Saver()               \n",
    "                saver.save(sess, \"%s\"%best_model_file )\n",
    "\n",
    "                #\n",
    "                best_loss = validation_mse\n",
    "                best_cost = validation_mse\n",
    "                best_r2 = validation_r2\n",
    "\n",
    "                \n",
    "    print \"Training time: \", time.time() - start\n",
    "    \n",
    "\n",
    "    return best_r2\n",
    "\n",
    "\n",
    "# Training\n",
    "best_dropout = 0.8  \n",
    "best_l2_coef = 0.0001 \n",
    "best_lr = 0.0005     \n",
    "\n",
    "\n",
    "print '\\n', training(_trX,_trX_rc, _trY, _vaX,_vaX_rc, _vaY, \\\n",
    "                    {'dropout_keep':best_dropout,'l2':best_l2_coef, 'lr':best_lr},\\\n",
    "        _epochs, _batch_size , _best_model_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:seq]",
   "language": "python",
   "name": "conda-env-seq-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
