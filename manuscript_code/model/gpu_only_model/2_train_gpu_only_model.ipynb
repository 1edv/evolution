{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65ddce0",
   "metadata": {},
   "source": [
    "# Please use the transformer model for your future applications instead of the convolutional model in this directory.\n",
    "# The transformer model is available at: https://github.com/1edv/evolution/tree/master/manuscript_code/model/tpu_model \n",
    "# For more details on why we recommend the transformer model instead of the model described here, please see: https://github.com/1edv/evolution/tree/master/manuscript_code/model/gpu_only_model#please-use-the-transformer-model-for-your-future-applications-instead-of-the-convolutional-model-in-this-directory \n",
    "## This notebook allows the user to train their own version of the GPU model from scratch\n",
    "- This notebook can also be run using the `2_train_gpu_model.py` file in this folder. \n",
    "\n",
    "\n",
    "#### Notes\n",
    "- The training data for training the GPU model uses a separate file format. We have also uploaded training data ( the one we used for the complex and defined media condition) in this format here so this training notebook will be fully functional on the Google Cloud VM and on CodeOcean(the file can be found in this directory and is used here below).\n",
    "\n",
    "- Caution : Saved models for each condition in the 'models_conditions' folder will be overwritten by running this code. We have a backup of the complex media model in the 'models' folder in case this happens.  As we have shown in the manuscript, the complex and defined media have highly correlated expression levels and doing the same for defined media will lead to equivalent prediction performance of the trained models.\n",
    "\n",
    "- <b>Also, please note the PCC metric shown on the 'validation set' is not any of the test data we use in the paper. It is simply a held-out sample of the training data experiment as we explain elsewhere as well. This training data is significantly higher complexity and hence lead to a much lower number of 'read replicates' per given sequence. So we carry out separate experiments low-complexity library experiments to measure the test data.</b> \n",
    "\n",
    "- We verified that training the model works on this machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd386c8",
   "metadata": {},
   "source": [
    "### Pre-process the training data for the GPU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6768ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/edv/anaconda3/envs/evolution/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "import multiprocessing as mp, ctypes\n",
    "import time , csv ,pickle ,joblib , matplotlib  , multiprocessing,itertools\n",
    "from joblib import Parallel, delayed \n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse,pwd,os,numpy as np,h5py\n",
    "from os.path import splitext,exists,dirname,join,basename\n",
    "from os import makedirs\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import tensorflow as tf, sys, numpy as np, h5py, pandas as pd\n",
    "from tensorflow import nn\n",
    "from tensorflow.contrib import rnn\n",
    "from os.path import join,dirname,basename,exists,realpath\n",
    "from os import makedirs\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import sklearn , scipy\n",
    "from sklearn.metrics import *\n",
    "from scipy.stats import *\n",
    "import time\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02021e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "################################################Final one used\n",
    "###GET ONE HOT CODE FROM SEQUENCES , parallel code, quite fast  \n",
    "class OHCSeq:\n",
    "    transformed = None\n",
    "    data = None\n",
    "\n",
    "\n",
    "def seq2feature(data):\n",
    "    num_cores = multiprocessing.cpu_count()-2\n",
    "    nproc = np.min([16,num_cores])\n",
    "    OHCSeq.data=data\n",
    "    shared_array_base = mp.Array(ctypes.c_bool, len(data)*len(data[0])*4)\n",
    "    shared_array = np.ctypeslib.as_array(shared_array_base.get_obj())\n",
    "    shared_array = shared_array.reshape(len(data),1,len(data[0]),4)\n",
    "    #OHCSeq.transformed = np.zeros([len(data),len(data[0]),4] , dtype=np.bool )\n",
    "    OHCSeq.transformed = shared_array\n",
    "\n",
    "\n",
    "    pool = mp.Pool(nproc)\n",
    "    r = pool.map(seq2feature_fill, range(len(data)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    #myOHC.clear()\n",
    "    return( OHCSeq.transformed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def seq2feature_fill(i):\n",
    "    mapper = {'A':0,'C':1,'G':2,'T':3,'N':None}\n",
    "    ###Make sure the length is 110bp\n",
    "    if (len(OHCSeq.data[i]) > 110) :\n",
    "        OHCSeq.data[i] = OHCSeq.data[i][-110:]\n",
    "    elif (len(OHCSeq.data[i]) < 110) : \n",
    "        while (len(OHCSeq.data[i]) < 110) :\n",
    "            OHCSeq.data[i] = 'N'+OHCSeq.data[i]\n",
    "    for j in range(len(OHCSeq.data[i])):\n",
    "        OHCSeq.transformed[i][0][j][mapper[OHCSeq.data[i][j]]]=True \n",
    "    return i\n",
    "\n",
    "########GET ONE HOT CODE FROM SEQUENCES , parallel code, quite fast  \n",
    "################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd56167",
   "metadata": {},
   "source": [
    "### Load the training data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a8c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conditions = 'defined' #'complex' or 'defined' \n",
    "with open('./'+model_conditions+'_media_training_data.txt') as f: #replace with the path to your raw data\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    d = list(reader)\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa93f1a",
   "metadata": {},
   "source": [
    "### Extract the sequences and appropriately attach the constant flanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0a9d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21037407/21037407 [00:13<00:00, 1574924.35it/s]\n"
     ]
    }
   ],
   "source": [
    "sequences = [di[0] for di in d]\n",
    "\n",
    "### Append N's if the sequencing output has a length different from 17+80+13 (80bp with constant flanks)\n",
    "for i in tqdm(range(0,len(sequences))) : \n",
    "    if (len(sequences[i]) > 110) :\n",
    "        sequences[i] = sequences[i][-110:]\n",
    "    if (len(sequences[i]) < 110) : \n",
    "        while (len(sequences[i]) < 110) :\n",
    "            sequences[i] = 'N'+sequences[i]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fc510",
   "metadata": {},
   "source": [
    "### Convert the sequences to one hot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b000be",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_sequences = seq2feature(np.asarray(sequences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f76a0",
   "metadata": {},
   "source": [
    "### Get the reverse complement of the sequence\n",
    "Improved this implementation to make it faster for the readers to run in a single notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ebb2c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21037407/21037407 [00:24<00:00, 872469.96it/s] \n"
     ]
    }
   ],
   "source": [
    "tab = str.maketrans(\"ACTGN\", \"TGACN\")\n",
    "\n",
    "def reverse_complement_table(seq):\n",
    "    return seq.translate(tab)[::-1]\n",
    "\n",
    "rc_sequences = [reverse_complement_table(seq) for seq in tqdm(sequences)]\n",
    "\n",
    "rc_onehot_sequences = seq2feature(np.array(rc_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2094c",
   "metadata": {},
   "source": [
    "### Extract the expression corresponding to the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71cfb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions = [di[1] for di in d]\n",
    "expressions = np.asarray(expressions)\n",
    "expressions = expressions.astype('float')  \n",
    "expressions = np.reshape(expressions , [-1,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5c9e2",
   "metadata": {},
   "source": [
    "### Split training data into two groups _trX and _vaX but please note that this is not the test data !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961437a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "total_seqs = len(onehot_sequences)\n",
    "_trX = onehot_sequences[int(total_seqs/10):]\n",
    "_trX_rc = rc_onehot_sequences[int(total_seqs/10):]\n",
    "_trY = expressions[int(total_seqs/10):]\n",
    "\n",
    "_vaX = onehot_sequences[0:int(total_seqs/10)]\n",
    "_vaX_rc = rc_onehot_sequences[0:int(total_seqs/10)]\n",
    "_vaY = expressions[0:int(total_seqs/10)]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fc443",
   "metadata": {},
   "source": [
    "### Define hyperparameters and specify location for saving the model\n",
    "We have saved an example for training to test the file in the user models folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b2bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "##MODEL FILE SAVING ADDRESSES AND MINIBATCH SIZES\n",
    "\n",
    "# Training\n",
    "best_dropout = 0.8  \n",
    "best_l2_coef = 0.0001 \n",
    "best_lr = 0.0005     \n",
    "\n",
    "\n",
    "_batch_size = 1024\n",
    "_hyper_train_size = 2000\n",
    "#_valid_size = 1024\n",
    "_hidden = 256\n",
    "_epochs = 5\n",
    "\n",
    "\n",
    "_best_model_file = join('models_conditions',model_conditions+'_media','best_model.ckpt')\n",
    "_best_model_file_hyper = join('models_conditions',model_conditions+'_media','hyper_search', 'best_model.ckpt')\n",
    "\n",
    "\n",
    "for _file  in [_best_model_file, _best_model_file_hyper]:\n",
    "    if not exists(dirname(_file)):\n",
    "        makedirs(dirname(_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a44d4a",
   "metadata": {},
   "source": [
    "### Define Model Architecutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf0bbe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "### MODEL ARCHITECTURE\n",
    "####################################################################\n",
    "####################################################################\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride=1):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x, stride=2, filter_size=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 2, 1],\n",
    "                        strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "def cross_entropy(y, y_real):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y, labels = y_real))\n",
    "\n",
    "def build_two_fc_layers(x_inp, Ws, bs):\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_inp, Ws[0]) + bs[0])\n",
    "    return tf.matmul(h_fc1, Ws[1]) + bs[1]\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "def cnn_model(X, hyper_params , scope):\n",
    "\n",
    "    with tf.variable_scope(scope) : \n",
    "        global _hidden \n",
    "        conv1_filter_dim1 = 30\n",
    "        conv1_filter_dim2 = 4\n",
    "        conv1_depth = _hidden\n",
    "\n",
    "        conv2_filter_dim1 = 30\n",
    "        conv2_filter_dim2 = 1\n",
    "        conv2_depth = _hidden\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        W_conv1 = weight_variable([1,conv1_filter_dim1,conv1_filter_dim2,conv1_depth])\n",
    "        conv1 = conv2d(X, W_conv1)    \n",
    "        conv1 = tf.nn.bias_add(conv1, bias_variable([conv1_depth]))\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        l_conv = conv1\n",
    "        \n",
    "        W_conv2 = weight_variable([conv2_filter_dim1,conv2_filter_dim2,conv1_depth, conv2_depth])\n",
    "        conv2 = conv2d(conv1,W_conv2 )\n",
    "        conv2 = tf.nn.bias_add(conv2, bias_variable([conv2_depth]))\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "        \n",
    "        regularization_term = hyper_params['l2']* tf.reduce_mean(tf.abs(W_conv1)) + hyper_params['l2']* tf.reduce_mean(tf.abs(W_conv2)) \n",
    "        \n",
    "        cnn_model_output = conv2\n",
    "\n",
    "    return cnn_model_output , regularization_term \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def training(trX, trX_rc, trY, valX, valX_rc, valY, hyper_params, epochs, batch_size, best_model_file): \n",
    "\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    global _hidden \n",
    "\n",
    "    lstm_num_hidden = _hidden\n",
    "    fc_num_hidden = _hidden\n",
    "    num_classes = 1\n",
    "    num_bins = 256\n",
    "    \n",
    "    conv3_filter_dim1 = 30\n",
    "    conv3_filter_dim2 = 1\n",
    "    conv3_depth = _hidden\n",
    "    \n",
    "    conv4_filter_dim1 = 30\n",
    "    conv4_filter_dim2 = 1\n",
    "    conv4_depth = _hidden\n",
    "        # Input and output\n",
    "\n",
    "    X = tf.placeholder(\"float\", [None, 1, 110, 4] )\n",
    "    X_rc = tf.placeholder(\"float\", [None, 1, 110, 4] )\n",
    "    Y = tf.placeholder(\"float\", [None,1] )\n",
    "    dropout_keep_probability = tf.placeholder_with_default(1.0, shape=())\n",
    "\n",
    "\n",
    "    #f is forward sequence \n",
    "    output_f , regularization_term_f =  cnn_model(X, {'dropout_keep':hyper_params['dropout_keep'],'l2':hyper_params['l2']} , \"f\")\n",
    "\n",
    "    #rc is reverse complement of that sequence\n",
    "    output_rc , regularization_term_rc =  cnn_model(X_rc, {'dropout_keep':hyper_params['dropout_keep'],'l2':hyper_params['l2']} , \"rc\")\n",
    "    \n",
    "    \n",
    "    ### CONCATENATE output_f and output_rc\n",
    "    concatenated_f_rc = tf.concat([output_f , output_rc], -1)\n",
    "    ###\n",
    "    \n",
    "    W_conv3 = weight_variable([conv3_filter_dim1,conv3_filter_dim2,2*_hidden,conv3_depth])\n",
    "    conv3 = conv2d(concatenated_f_rc,W_conv3 )\n",
    "    conv3 = tf.nn.bias_add(conv3, bias_variable([conv3_depth]))\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "    W_conv4 = weight_variable([conv4_filter_dim1,conv4_filter_dim2,conv3_depth,conv4_depth])\n",
    "    conv4 = conv2d(conv3,W_conv4 )\n",
    "    conv4 = tf.nn.bias_add(conv4, bias_variable([conv4_depth]))\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "\n",
    "\n",
    "    conv_feat_map_x = 110   \n",
    "    conv_feat_map_y =  1   \n",
    "    h_conv_flat = tf.reshape(conv4, [-1, conv_feat_map_x * conv_feat_map_y * lstm_num_hidden])\n",
    "    #FC-1\n",
    "    \n",
    "    W_fc1 = weight_variable([conv_feat_map_x * conv_feat_map_y * lstm_num_hidden , fc_num_hidden])\n",
    "    b_fc1 = bias_variable([fc_num_hidden])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv_flat, W_fc1) + b_fc1)\n",
    "    #Dropout for FC-1\n",
    "    h_fc1 = tf.nn.dropout(h_fc1, dropout_keep_probability)\n",
    "\n",
    "    \n",
    "    #FC-2\n",
    "    W_fc2 = weight_variable([fc_num_hidden , num_bins])\n",
    "    b_fc2 = bias_variable([num_bins])\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    #Dropout for FC-2\n",
    "    h_fc2 = tf.nn.dropout(h_fc2, dropout_keep_probability)\n",
    "\n",
    "\n",
    "    #FC-3\n",
    "    W_fc3 = weight_variable([num_bins, num_classes])\n",
    "    b_fc3 = bias_variable([num_classes])\n",
    "    h_fc3 = tf.matmul(h_fc2, W_fc3) + b_fc3 \n",
    "\n",
    "    regularization_term = hyper_params['l2']* tf.reduce_mean(tf.abs(W_fc3)) + hyper_params['l2']* tf.reduce_mean(tf.abs(W_fc2)) + hyper_params['l2']* tf.reduce_mean(tf.abs(W_fc1)) + hyper_params['l2']* tf.reduce_mean(tf.abs(W_conv3))+ regularization_term_f + regularization_term_rc +hyper_params['l2']* tf.reduce_mean(tf.abs(W_conv4))\n",
    "\n",
    "\n",
    "\n",
    "    with tf.variable_scope(\"out\") :\n",
    "        output = h_fc3\n",
    "        model_output = tf.identity(output, name=\"model_output\")\n",
    "\n",
    "        ##########\n",
    "\n",
    "        loss = tf.losses.mean_squared_error( Y , model_output ) + regularization_term\n",
    "        cost = loss \n",
    "        model_cost = tf.identity(cost, name=\"model_cost\")\n",
    "        ##########\n",
    "        pcc = tf.contrib.metrics.streaming_pearson_correlation(model_output,Y)\n",
    "        model_pcc = tf.identity(pcc, name=\"model_pcc\")\n",
    "        ##########\n",
    "        mse = tf.losses.mean_squared_error( Y , model_output )\n",
    "        model_mse = tf.identity(mse, name=\"model_mse\")\n",
    "        ##########\n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(Y, tf.reduce_mean(Y))))\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(Y, model_output)))\n",
    "        R_squared = tf.subtract(tf.constant(\n",
    "    1,\n",
    "    dtype=tf.float32), tf.div(unexplained_error, total_error))\n",
    "        model_R_squared = tf.identity(R_squared, name=\"model_R_squared\")\n",
    "\n",
    "        ##########\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    tf.summary.scalar(\"cost\", model_cost)\n",
    "    tf.summary.scalar(\"pcc\", model_pcc[0])\n",
    "    tf.summary.scalar(\"mse\", model_mse)\n",
    "    tf.summary.scalar(\"R_squared\", R_squared)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(hyper_params['lr']).minimize(cost)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    best_cost = float(\"inf\") \n",
    "    best_r2 = float(0) \n",
    "\n",
    "    \n",
    "    batches_per_epoch = int(len(trX)/batch_size)\n",
    "    num_steps = int(epochs * batches_per_epoch)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init=tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    #init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    #clear the logs directory\n",
    "    now = datetime.now()\n",
    "    #writer = tf.summary.FileWriter(join('user_models','logs' , now.strftime(\"%Y%m%d-%H%M%S\") ), sess.graph)\n",
    "\n",
    "    print('Initializing variables...')\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_pcc = 0\n",
    "    epoch_mse = 0\n",
    "    epoch_R_squared = 0\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    for step in tqdm(range(num_steps)):\n",
    "    \n",
    "    \n",
    "    \n",
    "        offset = (step * batch_size) % (trX.shape[0] - batch_size)\n",
    "        batch_x = trX[offset:(offset + batch_size), :]\n",
    "        batch_x_rc = trX_rc[offset:(offset + batch_size), :]\n",
    "        batch_y = trY[offset:(offset + batch_size)]\n",
    "\n",
    "        \n",
    "        feed_dict = {X: batch_x, X_rc: batch_x_rc ,  Y: batch_y , dropout_keep_probability : hyper_params['dropout_keep'] }\n",
    "        \n",
    "        _, batch_loss , batch_pcc , batch_mse, batch_R_squared , summary = sess.run([train_op, cost , pcc , mse, R_squared , summary_op], feed_dict=feed_dict)\n",
    "        \n",
    "        batch_R_squared = batch_pcc[0]**2\n",
    "        epoch_loss += batch_loss\n",
    "        epoch_pcc += batch_pcc[0]\n",
    "        epoch_mse += batch_mse        \n",
    "        epoch_R_squared += batch_R_squared    \n",
    "        \n",
    "        #writer.add_summary(summary, step)\n",
    "\n",
    "        \n",
    "        \n",
    "        if ( (step % batches_per_epoch == 0) and step/batches_per_epoch!=0):\n",
    "\n",
    "            epoch_loss /= batches_per_epoch\n",
    "            epoch_pcc /= batches_per_epoch\n",
    "            epoch_mse /= batches_per_epoch\n",
    "            epoch_R_squared /= batches_per_epoch\n",
    "            \n",
    "            print('')\n",
    "            print( '')\n",
    "            print( '')\n",
    "            print( '')\n",
    "            print( 'Training - Avg batch loss at epoch %d: %f' % (step/batches_per_epoch, epoch_loss))\n",
    "            print( 'Training - PCC : %f' % epoch_pcc)\n",
    "            print( 'Training - MSE : %f' % epoch_mse)\n",
    "            print( 'Training - R_squared : %f' % epoch_pcc**2)\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            epoch_pcc = 0\n",
    "            epoch_mse = 0\n",
    "            epoch_R_squared = 0  \n",
    "            \n",
    "            #Randomized validation subset start\n",
    "            randomize  =  np.random.permutation(len(valX))\n",
    "            vaX = valX[randomize,:]\n",
    "            vaX_rc = valX_rc[randomize,:]\n",
    "            vaY = valY[randomize,:]\n",
    "            \n",
    "            #valX = vaX[0:valid_size,:] \n",
    "            #valX_rc = vaX_rc[0:valid_size,:] \n",
    "            #valY = vaY[0:valid_size,:]            \n",
    "            \n",
    "            #with tf.device('/cpu:0'):\n",
    "            #validation_cost , validation_acc , summary = sess.run([cost , accuracy , summary_op], feed_dict={X: valX , X_rc: valX_rc, Y: valY})\n",
    "            \n",
    "            #### teX_output contains TESTED SEQUENCES FOR VALIDATION SET\n",
    "            va_batch_size = 1024\n",
    "            (q,r) = divmod(vaX.shape[0] , va_batch_size)\n",
    "            i=0\n",
    "            vaX_output = []\n",
    "            while(i <= q ) :\n",
    "                if(i< q  ) :\n",
    "                    temp_result_step1=sess.run([model_output], feed_dict={X: vaX[va_batch_size*i:va_batch_size*i+va_batch_size,:], X_rc: vaX_rc[va_batch_size*i:va_batch_size*i+va_batch_size,:]  ,Y: vaY[va_batch_size*i:va_batch_size*i+va_batch_size,:]}) \n",
    "                    temp_result_step2=[float(x) for x in temp_result_step1[0]]\n",
    "                    #print temp_result_step2\n",
    "\n",
    "                    vaX_output = vaX_output + temp_result_step2\n",
    "                    i = i+1\n",
    "\n",
    "                elif (i==q) :\n",
    "                    temp_result_step1 = sess.run([model_output], feed_dict={X: vaX[va_batch_size*i:,:], X_rc: vaX_rc[va_batch_size*i:,:]  ,Y: vaY[va_batch_size*i:,:]})\n",
    "                    temp_result_step2=[float(x) for x in temp_result_step1[0]]\n",
    "                    #print \"here\"\n",
    "                    vaX_output = vaX_output + temp_result_step2\n",
    "                    i = i+1\n",
    "\n",
    "            #### RETURN TESTED SEQUENCES FOR VALIDATION SET\n",
    "            vaY = [float(x) for x in vaY]\n",
    "            validation_mse = sklearn.metrics.mean_squared_error(vaY , vaX_output )\n",
    "\n",
    "            validation_pcc = scipy.stats.pearsonr(vaY , vaX_output )\n",
    "            validation_r2  = validation_pcc[0]**2\n",
    "        \n",
    "        \n",
    "        \n",
    "            #for tensorboard\n",
    "            print('')\n",
    "            print( 'Full Validation Set - MSE : %f' % validation_mse)\n",
    "            print( 'Full Validation Set - PCC : %f' % validation_pcc[0])\n",
    "            print( 'Full Validation Set - R_squared : %f' % validation_r2)\n",
    "\n",
    "            if(best_r2 < validation_r2) :\n",
    "                #\n",
    "                \n",
    "                #SAVER \n",
    "                saver = tf.train.Saver()               \n",
    "                saver.save(sess, \"%s\"%best_model_file )\n",
    "\n",
    "                #\n",
    "                best_loss = validation_mse\n",
    "                best_cost = validation_mse\n",
    "                best_r2 = validation_r2\n",
    "\n",
    "                \n",
    "    print( \"Training time: \", time.time() - start)\n",
    "    \n",
    "\n",
    "    return best_r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffa7e9",
   "metadata": {},
   "source": [
    "### Train the model. \n",
    "Note that the model autosaves in the path defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc598c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('\\n', training(_trX,_trX_rc, _trY, _vaX,_vaX_rc, _vaY, \\\n",
    "                    {'dropout_keep':best_dropout,'l2':best_l2_coef, 'lr':best_lr},\\\n",
    "        _epochs, _batch_size , _best_model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254b306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:evolution] *",
   "language": "python",
   "name": "conda-env-evolution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
