{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.argv[1]='SC_Ura'\n",
    "from aux import *\n",
    "\n",
    "\n",
    "##Clear Memory \n",
    "tf.reset_default_graph()\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "##\n",
    "\n",
    "\n",
    "NUM_GPU = len(get_available_gpus())\n",
    "if(NUM_GPU>0) :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "if sys.argv[1]=='Glu' :\n",
    "    tpu_grpc_url = TPUClusterResolver(tpu=['edv-tpu3'] , zone='us-central1-a').get_master()\n",
    "if(sys.argv[1] == 'SC_Ura') : \n",
    "    tpu_grpc_url = TPUClusterResolver(tpu=['edv-tpu1'] , zone='us-central1-a').get_master()\n",
    "\n",
    "#sys.argv[1] appended to beginning of path\n",
    "dir_path=os.path.join('..','..','data',sys.argv[1])\n",
    "model_path=os.path.join(dir_path,\"fitness_function.h5\")\n",
    "\n",
    "\n",
    "###\n",
    "model_conditions = sys.argv[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Load the data matrix\n",
    "with h5py.File(join(dir_path,'_trX.h5'), 'r') as hf:\n",
    "    _trX = hf['_trX'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_trY.h5'), 'r') as hf:\n",
    "    _trY = hf['_trY'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_vaX.h5'), 'r') as hf:\n",
    "    _vaX = hf['_vaX'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_vaY.h5'), 'r') as hf:\n",
    "    _vaY = hf['_vaY'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_teX.h5'), 'r') as hf:\n",
    "    _teX = hf['_teX'][:]\n",
    "\n",
    "with h5py.File(join(dir_path,'_teY.h5'), 'r') as hf:\n",
    "    _teY = hf['_teY'][:]\n",
    "\n",
    "\n",
    "_trX.shape , _trY.shape , _vaX.shape , _vaY.shape  , _teX.shape , _teY.shape\n",
    "\n",
    "\n",
    "\n",
    "trX = _trX #np.concatenate((_trX, _trX_rc), axis = 1) #np.squeeze((_trX))#\n",
    "vaX = _vaX # np.concatenate((_vaX, _vaX_rc), axis = 1) #np.squeeze((_vaX))#\n",
    "teX = _teX # np.concatenate((_teX, _teX_rc), axis = 1)#np.squeeze((_teX))#\n",
    "\n",
    "\n",
    "\n",
    "## Load the scaler function (scaler was trained on the synthesized data\n",
    "scaler = sklearn.externals.joblib.load(join(dir_path,'scaler.save')) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vaY = (scaler.transform(_vaY.reshape(1, -1))).reshape(_vaY.shape) #_vaY#\n",
    "trY = (scaler.transform(_trY.reshape(1, -1))).reshape(_trY.shape) #_trY#\n",
    "teY = (scaler.transform(_teY.reshape(1, -1))).reshape(_teY.shape) #_teY#\n",
    "\n",
    "\n",
    "### If using generator, have a smaller val set for faster evaluation\n",
    "if 0 : \n",
    "    s_trX = np.vstack((trX , vaX))\n",
    "    s_trY = np.vstack((trY , vaY))\n",
    "    trX = s_trX[100000:,:]\n",
    "    trY = s_trY[100000:,:]\n",
    "    vaX = s_trX[0:100000,:]\n",
    "    vaY = s_trY[0:100000,:]\n",
    "\n",
    "print(trX.shape , trY.shape , vaX.shape , vaY.shape  , _teX.shape , _teY.shape)\n",
    "\n",
    "input_shape = trX.shape\n",
    "\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    'n_val_epoch' : 1000,\n",
    "    'epochs' : 100,\n",
    "    'batch_size': int(1024*1), # int(1024*3) , #64*55 fits , #best batch size is 1024\n",
    "    'l1_weight': 0,#1e-6#1e-7#0.01 # l1 should always be zero\n",
    "    'l2_weight': 1e-4,#1e-7#0.01\n",
    "    'motif_conv_hidden': 256,\n",
    "    'conv_hidden': 128,\n",
    "    'n_hidden': 128, #128\n",
    "    'n_heads': 8,\n",
    "    'conv_width_motif':30, ##30bp for yeast is required for capturing all motifs \n",
    "    'dropout_rate': 0.2,\n",
    "    'lr':0.001,\n",
    "    'add_cooperativity_layer': True,\n",
    "    'n_aux_layers': 2,\n",
    "    'n_attention_layers':2,\n",
    "    'attention_dropout_rate' : 0.2,\n",
    "    'device_type' : 'tpu', #'tpu'/'gpu'/'cpu'\n",
    "    'input_shape' : input_shape,\n",
    "    'loss' : 'mean_squared_error'}\n",
    "epochs = model_params['epochs']  \n",
    "batch_size =  model_params['batch_size']\n",
    "n_val_epoch =  model_params['n_val_epoch']\n",
    "epochs = model_params['epochs']\n",
    "\n",
    "### Save model params as csv\n",
    "w = csv.writer(open(os.path.join(dir_path,'model_params.csv'), \"w\"))\n",
    "for key, val in model_params.items():\n",
    "    w.writerow([key, val])\n",
    "\n",
    "### Save model params as pickle\n",
    "f = open(os.path.join(dir_path,'model_params.pkl'),\"wb\")\n",
    "pickle.dump(model_params,f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=fitness_function_model(model_params)\n",
    "if 1 :\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    #plot_model(model, show_shapes = 1 , show_layer_names = 1 ,to_file='model.png')\n",
    "\n",
    "    #'Nadam'\n",
    "\n",
    "    if(NUM_GPU > 1) : \n",
    "        model = multi_gpu_model(model, gpus=NUM_GPU, cpu_merge=True, cpu_relocation=False)\n",
    "\n",
    "\n",
    "    if(tpu_grpc_url) : \n",
    "        model = tf.contrib.tpu.keras_to_tpu_model(model,\n",
    "            strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "                tf.contrib.cluster_resolver.TPUClusterResolver(tpu_grpc_url)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Randomly select dataset for the generator for the common model\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    #https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, shuffle=True ):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        if self.shuffle == True:\n",
    "            return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "        else:\n",
    "            return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        if self.shuffle == True:\n",
    "            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        else:\n",
    "            indexes = self.indexes[index*self.batch_size:np.minimum(len(self.list_IDs),(index+1)*self.batch_size)]    \n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        batch_X , batch_Y = self.__data_generation(list_IDs_temp)\n",
    "        return  (batch_X , batch_Y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp ):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        batch_X = trX[list_IDs_temp]\n",
    "        batch_Y = trY[list_IDs_temp]\n",
    "\n",
    "\n",
    "        return batch_X , batch_Y\n",
    "    \n",
    "\n",
    "if 0 : \n",
    "    params_train = {\n",
    "              'batch_size': batch_size,\n",
    "              'shuffle': True,\n",
    "              }\n",
    "    len_list_IDs = trX.shape[0]\n",
    "\n",
    "    # Datasets\n",
    "    partition = {'train': np.arange(len_list_IDs) }\n",
    "    training_generator = DataGenerator(partition['train'], **params_train)\n",
    "    example_batch = training_generator[0]\n",
    "\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "history = LossHistory()\n",
    "\n",
    "csvlogger = keras.callbacks.CSVLogger(os.path.join(dir_path,\"loss_history.tsv\"), separator='\\t', append=False)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=os.path.join(dir_path,\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")),update_freq='batch', histogram_freq=0, batch_size=batch_size, write_graph=True, \n",
    "                            write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, \n",
    "                            embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(model_path, monitor='val_r_square', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# check 5 epochs\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_r_square', patience=10, mode='max') \n",
    "\n",
    "callbacks_list = [checkpoint, early_stop, history , tensorboard , csvlogger]\n",
    "\n",
    "\n",
    "model.fit(trX, trY, validation_data = (vaX, vaY),\n",
    "          batch_size=batch_size  , epochs=epochs , callbacks=callbacks_list )\n",
    "\n",
    "#model.fit_generator(training_generator, validation_data = (vaX, vaY),\n",
    "#epochs=epochs , steps_per_epoch = int(trX.shape[0]/(batch_size*n_val_epoch)) , callbacks=callbacks_list )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 0:\n",
    "    model = load_model('keras_model.h5' , custom_objects={\n",
    "        'MultiHeadAttention' : MultiHeadAttention , \n",
    "        'FeedForward' : FeedForward,\n",
    "        'correlation_coefficient' : correlation_coefficient,\n",
    "        'LayerNormalization' : LayerNormalization,\n",
    "        'rc_Conv1D' : rc_Conv1D})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
